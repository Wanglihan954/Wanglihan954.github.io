<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>宁翰のHEXO</title>
  
  <subtitle>我的世界</subtitle>
  <link href="https://20020730.xyz/atom.xml" rel="self"/>
  
  <link href="https://20020730.xyz/"/>
  <updated>2025-12-12T16:53:51.816Z</updated>
  <id>https://20020730.xyz/</id>
  
  <author>
    <name>宁翰</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Github学生优惠获取教程</title>
    <link href="https://20020730.xyz/posts/70d8eb1/"/>
    <id>https://20020730.xyz/posts/70d8eb1/</id>
    <published>2025-12-12T13:31:43.000Z</published>
    <updated>2025-12-12T16:53:51.816Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer@latest/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer@latest/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting@1/dist/Meting.min.js"></script><h1 id="Github学生优惠获取教程"><a href="#Github学生优惠获取教程" class="headerlink" title="Github学生优惠获取教程"></a>Github学生优惠获取教程</h1>    <div id="aplayer-ZVqKYZpl" class="aplayer aplayer-tag-marker meting-tag-marker"         data-id="497572729" data-server="netease" data-type="song" data-mode="circulation" data-autoplay="false" data-mutex="true" data-listmaxheight="340px" data-preload="auto" data-theme="#C20C0C"    ></div><p>步骤(1-4 推荐使用电脑操作)</p><h2 id="填写信息-可以使用VPN"><a href="#填写信息-可以使用VPN" class="headerlink" title="填写信息(可以使用VPN)"></a>填写信息(可以使用VPN)</h2><ol><li>点击头像，选择<code>settings</code><ol><li>可选菜单：Set status、Profile、Repositories、Stars、Gists、Organizations、Enterprises、Sponsors</li><li>进入Settings后可见：Copilot settings、Feature preview（New）、Appearance、Accessibility、Try Enterprise Free、Sign out、Public profile</li></ol></li><li>填写公开资料信息<ol><li>名字填写要求：姓在后名在前，如:HuaLi</li><li>需填写学校官网网址</li></ol></li></ol><p><img src="https://cdn.jsdelivr.net/gh/Wanglihan954/Picture-bed@img/img/image-20251212214512055.png" alt="image-20251212214512055" loading="lazy"></p><h2 id="完善账单信息-可以使用VPN"><a href="#完善账单信息-可以使用VPN" class="headerlink" title="完善账单信息(可以使用VPN)"></a>完善账单信息(可以使用VPN)</h2><ol><li>进入账单信息页面：在设置中找到<code>Billing information</code>（账单信息），入口路径包含Public profile（公开资料）、Payment information（支付信息）、Account（账户）、Appearance（外观）等多个设置分类</li><li>填写账单信息项<ol><li>姓名：First name（名）、Last name（姓），示例为Hua Li</li><li>地址（Street, P.O.box）：填写大学英文名，可重复填写，示例为The University of Tokyo</li><li>Address line 2（Apartment, suite, unit）：无示例，按需填写</li><li>City（城市）：填写英文，示例为Tokyo</li><li>Country&#x2F;Region（国家&#x2F;地区）：可选项如Japan，国内用户选China</li><li>State&#x2F;Province（州&#x2F;省）：按需填写</li><li>Postal&#x2F;Zip code（邮政编码）：部分国家&#x2F;地区必填（美国需9位邮编）</li><li>VAT&#x2F;GSTID（增值税&#x2F;商品及服务税识别号）：按需填写</li></ol></li><li>填写完成后点击<code>Save billing information</code>（保存账单信息）</li></ol><p><img src="https://cdn.jsdelivr.net/gh/Wanglihan954/Picture-bed@img/img/image-20251212214544933.png" alt="image-20251212214544933" loading="lazy"></p><h2 id="材料准备"><a href="#材料准备" class="headerlink" title="材料准备"></a>材料准备</h2><p>需准备<strong>Student Verification Report</strong>（学生验证报告），内容需包含以下信息，填写个人实际内容即可：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Student Verification Report</span><br><span class="line">Name : Han Ning</span><br><span class="line">School : Peking University</span><br><span class="line">Student Number : 123456789</span><br><span class="line">Graduation Date :June 20, 2028</span><br><span class="line">Study Form: Full-time</span><br></pre></td></tr></table></figure><p>（注：文档中出现两份仅学号有差异的报告模板，实际准备一份个人真实报告即可）</p><h2 id="学生邮箱添加-可以使用VPN"><a href="#学生邮箱添加-可以使用VPN" class="headerlink" title="学生邮箱添加(可以使用VPN)"></a>学生邮箱添加(可以使用VPN)</h2><ol><li>进入邮箱设置板块，若已添加学校邮箱但未显示通过，需进行验证</li><li>操作入口：找到<code>Add email address</code>（添加邮箱地址）选项，录入学校邮箱并完成学校邮箱验证<ol><li>示例邮箱格式：<a href="mailto:&#x32;&#48;&#50;&#x33;&#48;&#x30;&#48;&#x31;&#64;&#112;&#x6b;&#x75;&#46;&#101;&#x64;&#x75;&#x2e;&#x63;&#x6e;">20230001@pku.edu.cn</a>（仅为格式参考，需替换为真实学校邮箱）</li></ol></li></ol><p><img src="https://cdn.jsdelivr.net/gh/Wanglihan954/Picture-bed@img/img/image-20251212214904510.png" alt="image-20251212214904510" loading="lazy"></p><h2 id="双因素认证-可以使用VPN"><a href="#双因素认证-可以使用VPN" class="headerlink" title="双因素认证(可以使用VPN)"></a>双因素认证(可以使用VPN)</h2><ol><li><p>进入双因素认证设置入口：在设置中找到<code>Password and authentication</code>（密码和认证）分类下的<code>Two-factor authentication</code>（双因素认证）</p></li><li><p>认证流程</p><ol><li>进入后会显示二维码，需在手机上下载<strong>Authenticator</strong>类身份验证App（如Microsoft Authenticator）</li></ol><p><img src="https://cdn.jsdelivr.net/gh/Wanglihan954/Picture-bed@img/img/image-20251212215007038.png" alt="image-20251212215007038" loading="lazy"></p><ol><li>打开App扫码完成验证即可启用双因素认证，增强账户安全性</li></ol></li></ol><p><img src="https://cdn.jsdelivr.net/gh/Wanglihan954/Picture-bed@img/img/image-20251212214934108.png" alt="image-20251212214934108" loading="lazy"></p><h2 id="开始申请-开始使用手机进行申请"><a href="#开始申请-开始使用手机进行申请" class="headerlink" title="开始申请(开始使用手机进行申请)"></a>开始申请(开始使用手机进行申请)</h2><p>注意：此步骤全程<strong>不能使用VPN</strong>，且需关闭校园网WiFi，仅使用国内流量；因GitHub IP限制，可能出现登录不畅，可多尝试几次</p><ol><li>进入教育福利申请页面，选择教育身份：<code>Student</code>（学生，勿选Teacher）</li><li>若已完成学校邮箱验证，页面会显示验证状态，确认学校选择后点击<code>Continue</code></li><li>选择证明类型（&#x3D;&#x3D;需选择b&#x3D;&#x3D;），可选类型如下：<ol><li>注明日期的学校证件-完整</li><li>Dated official&#x2F;unofficial transcript -Fair</li><li>Dated enrollment letter on school letterhead -Fair</li><li>Dated class schedule for the semester-Poor</li><li>Dated syllabus for a class - Poor</li><li>Dated receipt from bursar（可在此选项下上传证明材料照片，仅需拍摄文字材料部分）</li></ol></li><li><strong>材料上传&#x2F;提交完成即可，期间网站可能出现访问不畅，属于正常现象，可多次尝试</strong></li></ol><p><img src="https://cdn.jsdelivr.net/gh/Wanglihan954/Picture-bed@img/img/image-20251212215107668.png" alt="image-20251212215107668" loading="lazy"></p><h2 id="申请状态显示"><a href="#申请状态显示" class="headerlink" title="申请状态显示"></a>申请状态显示</h2><h3 id="审核中"><a href="#审核中" class="headerlink" title="审核中"></a>审核中</h3><p>页面显示如下状态，代表申请已提交待审核：</p><p><img src="https://cdn.jsdelivr.net/gh/Wanglihan954/Picture-bed@img/img/image-20251212215127824.png" alt="image-20251212215127824" loading="lazy"></p><h3 id="审核成功"><a href="#审核成功" class="headerlink" title="审核成功"></a>审核成功</h3><p>一般等待2-5分钟可完成审核，审核通过后页面显示如下：（Penging变为Approved）</p><p><img src="https://cdn.jsdelivr.net/gh/Wanglihan954/Picture-bed@img/img/image-20251212215152541.png" alt="image-20251212215152541" loading="lazy"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;https://cdn.jsdelivr.net/npm/aplayer@latest/dist/APlayer.min.css&quot;&gt;&lt;scrip</summary>
      
    
    
    
    
    <category term="Agent" scheme="https://20020730.xyz/tags/Agent/"/>
    
    <category term="Github" scheme="https://20020730.xyz/tags/Github/"/>
    
    <category term="LLM" scheme="https://20020730.xyz/tags/LLM/"/>
    
  </entry>
  
  <entry>
    <title>Lesson 1</title>
    <link href="https://20020730.xyz/posts/6a42c494/"/>
    <id>https://20020730.xyz/posts/6a42c494/</id>
    <published>2025-12-03T12:00:39.000Z</published>
    <updated>2025-12-11T04:25:58.014Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer@latest/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer@latest/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting@1/dist/Meting.min.js"></script><h1 id="第一课-深度学习推理框架基础"><a href="#第一课-深度学习推理框架基础" class="headerlink" title="第一课 深度学习推理框架基础"></a>第一课 深度学习推理框架基础</h1><h2 id="什么是推理框架"><a href="#什么是推理框架" class="headerlink" title="什么是推理框架"></a>什么是推理框架</h2><p>深度学习推理框架用于&#x3D;&#x3D;完成训练阶段的神经网络模型文件&#x3D;&#x3D;进行加载，并根据模型文件中<strong>网络结构</strong>与<strong>权重参数</strong>对输入图像（数据）进行预测。</p><h2 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h2><h3 id="Windows-下的环境准备"><a href="#Windows-下的环境准备" class="headerlink" title="Windows 下的环境准备"></a><strong>Windows 下的环境准备</strong></h3><ol><li>拉取Docker镜像：</li></ol><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker pull registry.cn-hangzhou.aliyuncs.com/hellofss/kuiperinfer:datawhale</span><br></pre></td></tr></table></figure><ol start="2"><li>创建本地文件夹，并将课程代码克隆到该文件夹中。</li></ol><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/zjhellofss/kuiperdatawhale.git</span><br></pre></td></tr></table></figure><ol start="3"><li>创建并运行一个镜像的容器。</li></ol><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -it -p 7860:22 registry.cn-hangzhou.aliyuncs.com/hellofss/kuiperinfer:datawhale /bin/bash</span><br></pre></td></tr></table></figure><ol start="4"><li>尝试使用ssh命令连接容器，这里的<strong>用户名固定是<code>me</code>, 登录密码是1.</strong></li></ol><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh -p 7860 me@127.0.0.1</span><br></pre></td></tr></table></figure><h3 id="Linux下环境准备"><a href="#Linux下环境准备" class="headerlink" title="Linux下环境准备"></a>Linux下环境准备</h3><ol><li><p>拉取<code>docker</code>镜像：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo docker pull registry.cn-hangzhou.aliyuncs.com/hellofss/kuiperinfer:datawhale</span><br></pre></td></tr></table></figure></li><li><p>创建本地文件夹，并克隆课程代码</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p ~/code/kuiperdatawhale</span><br><span class="line">cd ~/code/kuiperdatawhale</span><br><span class="line">git clone https://github.com/zjhellofss/kuiperdatawhale.git</span><br></pre></td></tr></table></figure></li><li><p>创建一个镜像的容器并运行。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo docker run -it registry.cn-hangzhou.aliyuncs.com/hellofss/kuiperinfer:datawhale </span><br><span class="line">/bin/bash</span><br></pre></td></tr></table></figure></li><li><p>在容器中输入<code>ifconfig</code>查看ip地址。</p></li><li><p>尝试使用ssh命令连接容器，这里的用户名固定是me，ip地址为4中<code>ifconfig</code>输出的inet，登录密码设置为1</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh me@inet</span><br></pre></td></tr></table></figure></li></ol><h2 id="关于KuiperInfer的技术全景概述"><a href="#关于KuiperInfer的技术全景概述" class="headerlink" title="关于KuiperInfer的技术全景概述"></a>关于KuiperInfer的技术全景概述</h2><p><code>KuiperInfer</code>可以分为以下几个部分：</p><ol><li><code>Operator</code>:深度学习计算图中的计算节点。包括以下几个部分：<ol><li>存储输入与输出张量。</li><li>计算节点的类型与名称</li><li>参数信息（卷积核的步长，大小）</li><li>权重信息（weight，bias）</li></ol></li><li><code>Graph</code>:多个<code>Operator</code>串联成的有向无环图，规定各个<code>Operato</code>的执行流程与顺序。</li><li><code>Layer</code>:<code>Operator</code>中运行的<strong>具体执行者</strong>。</li><li><code>Tensor</code>:用于存储&#x3D;&#x3D;多维数据&#x3D;&#x3D;的数据结构，方便数据在计算节点之间传递，同时该结构也封装矩阵乘、点积等与矩阵相关的基本操作。</li></ol><p><img src="https://pic2.zhimg.com/v2-504deae23f3f51a0a6597e07009bd0b5_r.jpg" alt="img" loading="lazy"></p><h2 id="使用VScode连接容器"><a href="#使用VScode连接容器" class="headerlink" title="使用VScode连接容器"></a>使用VScode连接容器</h2><p>在 VSCode 中，连接容器是一件很简单的事情，首先你需要安装 Docker 插件，点击左侧的扩展栏（或是ctrl+shift+X），键入 Docker，随后选择安装：</p><p><img src="https://picx.zhimg.com/v2-85c4986be9d45ce8eb8b4267a01abd0d_1440w.jpg" alt="img" loading="lazy"></p><p>安装后，你将会看到左侧出现一个相同长相的小鲸鱼，左键点击它你将会看到我们启动的所有容器：</p><p><img src="https://pic4.zhimg.com/v2-cb71572f3884490e14187a0477168b41_1440w.jpg" alt="img" loading="lazy"></p><p>在成功进入容器之前，我们还需要再安装几个小插件，我们接着搜索 <code>Dev container</code> 以及 <code>Remote Development</code> 插件，都安装第一个即可。</p><ol><li>接下来，你需要右键之前启动的容器 <code>registry.cn-hangzhou.aliyuncs.com/hellofss/kuiperinfer:datawhale</code>，选择 <strong>附加 Visual Studio Code</strong> 进入 Docker 环境，若你没有看到这个选项，请再次确保前面的三个插件已经都完全安装。<br>在进入 Docker 环境后，我们还需要手动再次 clone 课程代码，你可以执行 <code>ctrl + J</code> 调出控制台终端，在终端中输入：</li></ol><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /home </span><br><span class="line">git clone https://github.com/zjhellofss/kuiperdatawhale.git</span><br></pre></td></tr></table></figure><p>然后在左上角选择文件 —— 打开文件夹 —— &#x2F;home&#x2F;kuiperdatawhale —— 确定即可进入课程主界面。</p><ol start="2"><li>进入 Docker 环境后的 VSCode 拥有独立的插件环境，所以我们需要重新安装有关插件：<br>你需要在此时的主界面找到扩展，且根据安装 Docker 插件的步骤自行搜索完成以下几款插件的安装：<br>CMake 相关插件的安装（共三种）</li></ol><p><img src="https://pic4.zhimg.com/v2-5cf9a46adc0f7bf34020db00d0d9bd03_1440w.jpg" alt="img" loading="lazy"></p><p>C++ 相关插件的安装（共一种）</p><p><img src="https://pic2.zhimg.com/v2-1d9a6e43f4cb92e8794c2cb9a601d803_1440w.jpg" alt="img" loading="lazy"></p><ol start="3"><li>在安装成功后，我们就可以开始编译调试了，你将会在下方看到如下界面：</li></ol><p><img src="https://pic2.zhimg.com/v2-0e42a875cf878c92c79cb3da977e0c11_1440w.jpg" alt="img" loading="lazy"></p><p>首先，你需要点击工具图表，选择 <code>GCC 9.4.0 x86_64-linux-gnu</code> 版本的编译工具，确保编译工具一致。<br>接下来让我们尝试编译运行主程序，只需要在 ▶ 键右侧选择对应的编译目标（比如图中我选择了 <code>kuiper_datawhale_course1</code>），再点击▶ 按钮 即可开始编译运行。</p><p><img src="https://picx.zhimg.com/v2-8a1f6dfd4d2a1b5270d6b74e2e39bc45_1440w.jpg" alt="img" loading="lazy"></p><p>如果配置成功，程序会出现如下的运行结果，出现<code>Failed</code>这是因为本节课作业需要完成一定的代码，若作业都正确完成，<code>Failed</code>标签就会在运行结果中消失。</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[==========] Running 7 tests from 1 test suite.</span><br><span class="line">[----------] Global test environment set-up.</span><br><span class="line">[----------] 7 tests from test_arma</span><br><span class="line">[ RUN      ] test_arma.Axby</span><br><span class="line">/tmp/tmp.dvfnZylz2q/course1/test/axby.cpp:32: Failure</span><br><span class="line">Expected equality of these values:</span><br><span class="line">  approx_equal(y, answer, &quot;absdiff&quot;, 1e-5f)</span><br><span class="line">    Which is: false</span><br><span class="line">  true</span><br><span class="line">[  FAILED  ] test_arma.Axby (0 ms)</span><br><span class="line">[ RUN      ] test_arma.e_power_minus</span><br></pre></td></tr></table></figure><p>除了编译程序，调试程序也是很重要的一环，在这里你可以试试看点击 ▶ 键旁边的小虫子键，然后观察会发生什么现象，也可以自己尝试打断点看看程序的执行流是否会停留在预期位置，甚至在调试控制台输入变量看看会出现什么结果，这里的探索就交给你。另外，如果你想了解更多有关 VSCode 的使用方法以及调试说明，请参考<a href="https://link.zhihu.com/?target=https://code.visualstudio.com/docs/editor/debugging">资料</a>。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;https://cdn.jsdelivr.net/npm/aplayer@latest/dist/APlayer.min.css&quot;&gt;&lt;scrip</summary>
      
    
    
    
    <category term="KuiperInfer学习笔记" scheme="https://20020730.xyz/categories/KuiperInfer%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="C#" scheme="https://20020730.xyz/tags/C/"/>
    
    <category term="DeepLearning" scheme="https://20020730.xyz/tags/DeepLearning/"/>
    
  </entry>
  
  <entry>
    <title>Lesson 4</title>
    <link href="https://20020730.xyz/posts/5d9c34a6/"/>
    <id>https://20020730.xyz/posts/5d9c34a6/</id>
    <published>2025-12-03T12:00:39.000Z</published>
    <updated>2025-12-11T04:26:25.380Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer@latest/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer@latest/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting@1/dist/Meting.min.js"></script><h1 id="第四课-计算图的构建"><a href="#第四课-计算图的构建" class="headerlink" title="第四课 计算图的构建"></a>第四课 计算图的构建</h1><h2 id="拓扑排序"><a href="#拓扑排序" class="headerlink" title="拓扑排序"></a>拓扑排序</h2><p><img src="https://pica.zhimg.com/v2-37ed6844956354c8bad07d70723a1794_1440w.jpg" alt="img" loading="lazy"></p><p>对于一个有向无环图，拓扑排序总能够找到一个 <strong>节点序列</strong>，在这个序列中，<strong>每个节点的前驱节点都能排在这个节点的前面</strong>。什么是 <a href="https://zhida.zhihu.com/search?content_id=233781267&content_type=Article&match_order=2&q=%E5%89%8D%E9%A9%B1%E8%8A%82%E7%82%B9&zhida_source=entity">前驱节点</a> 呢，也就是对于 <strong>有向图中任意一条边的起点，我们可以认为它是 <a href="https://zhida.zhihu.com/search?content_id=233781267&content_type=Article&match_order=1&q=%E7%BB%88%E7%82%B9%E8%8A%82%E7%82%B9&zhida_source=entity">终点节点</a> 的前驱节点。</strong></p><p>对于上图例子中的深度学习模型，<code>conv1</code> 是 <code>conv2</code> 的前驱节点，<code>conv1</code> 也是 <code>conv3</code> 的前驱节点，所以在计算执行节点序列时，<code>conv1</code> 必须出现在 <code>conv2</code> 和 <code>conv3</code> 的前面，而 <code>conv2</code> 和 <code>conv3</code> 之间的顺序因为没有节点之间的连接而不做要求。因此执行节点顺序有以下两种：</p><ul><li>conv1-&gt; conv2-&gt; conv3</li><li>conv1-&gt; conv3-&gt; conv2</li></ul><h2 id="基于深度优先的拓扑排序计算步骤"><a href="#基于深度优先的拓扑排序计算步骤" class="headerlink" title="基于深度优先的拓扑排序计算步骤"></a>基于深度优先的拓扑排序计算步骤</h2><p>有计算排序的函数为 <code>ReverseTopo</code>. <code>ReverseTopo</code> 有参数 <code>current_op</code>.</p><ol><li>选定一个入度为零的节点(<code>current_op</code>)，入度为零指的是 <strong>该节点没有前驱节点或所有前驱节点已经都被执行过</strong>，在选定的同时将该节点的已执行标记置为 <code>True</code>，并将该节点传入到 <code>ReverseTopo</code> 函数中；</li><li>遍历 1 步骤中节点的 <a href="https://zhida.zhihu.com/search?content_id=233781267&content_type=Article&match_order=1&q=%E5%90%8E%E7%BB%A7%E8%8A%82%E7%82%B9&zhida_source=entity">后继节点</a>(<code>current_op-&gt;output_operators</code>)；</li><li>如果 1 的某个后继节点没有被执行过（已执行标记为 <code>False</code>），则 <a href="https://zhida.zhihu.com/search?content_id=233781267&content_type=Article&match_order=1&q=%E9%80%92%E5%BD%92&zhida_source=entity">递归</a> 将 <strong>该后继节点</strong> 传入到 <code>ReverseTopo</code> 函数中；</li><li>第 2 步中的遍历结束后，我们将当前节点放入到执行队列(<code>topo_operators_</code>)中。</li></ol><p>当该函数结束后，我们对 <strong><a href="https://zhida.zhihu.com/search?content_id=233781267&content_type=Article&match_order=2&q=%E6%89%A7%E8%A1%8C%E9%98%9F%E5%88%97&zhida_source=entity">执行队列</a> 中的排序结果做逆序就得到了最终拓扑排序的结果</strong>，我们来看看具体的代码：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">RuntimeGraph::ReverseTopo</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> std::shared_ptr&lt;RuntimeOperator&gt;&amp; current_op)</span> </span>&#123;</span><br><span class="line">  <span class="built_in">CHECK</span>(current_op != <span class="literal">nullptr</span>) &lt;&lt; <span class="string">&quot;current operator is nullptr&quot;</span>;</span><br><span class="line">  current_op-&gt;has_forward = <span class="literal">true</span>;</span><br><span class="line">  <span class="type">const</span> <span class="keyword">auto</span>&amp; next_ops = current_op-&gt;output_operators;</span><br><span class="line">  <span class="keyword">for</span> (<span class="type">const</span> <span class="keyword">auto</span>&amp; [_, op] : next_ops) &#123;</span><br><span class="line">    <span class="keyword">if</span> (op != <span class="literal">nullptr</span>) &#123;</span><br><span class="line">      <span class="keyword">if</span> (!op-&gt;has_forward) &#123;</span><br><span class="line">        <span class="keyword">this</span>-&gt;<span class="built_in">ReverseTopo</span>(op);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">for</span> (<span class="type">const</span> <span class="keyword">auto</span>&amp; [_, op] : next_ops) &#123;</span><br><span class="line">    <span class="built_in">CHECK_EQ</span>(op-&gt;has_forward, <span class="literal">true</span>);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">this</span>-&gt;topo_operators_.<span class="built_in">push_back</span>(current_op);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="一个复杂点的例子"><a href="#一个复杂点的例子" class="headerlink" title="一个复杂点的例子"></a>一个复杂点的例子</h3><p><img src="https://pic4.zhimg.com/v2-e20ad84bd6a3b0c9198388b30766756f_1440w.jpg" alt="img" loading="lazy"></p><p>对于上图的这个例子，<code>input</code> 节点是其中唯一的输入节点，<code>output</code> 节点是其中唯一的输出节点，其他都是 <a href="https://zhida.zhihu.com/search?content_id=233781267&content_type=Article&match_order=7&q=%E8%AE%A1%E7%AE%97%E8%8A%82%E7%82%B9&zhida_source=entity">计算节点</a>（<code>RuntieOperator</code>），表示深度学习模型中的一个算子。我们下面就对这个模型进行拓扑排序来求得它正确的执行顺序。要注意的是，<strong>拓扑排序可以有多种结果的序列，不同种序列之间的顺序也可能不完全相同，但是不同种序列均不会影响模型最终的预测输出。</strong></p><p>我们对上图的深度学习模型进行手工计算，计算步骤符合上一小节中的代码 <code>ReverseTopo</code> 函数。</p><ol><li>首选，传入到 <code>ReverseTopo</code> 函数中的是 <code>input</code> 节点；</li><li>对 <code>input</code> 节点进行遍历，<code>input</code> 节点的后继节点中没有执行过的只有 <code>op1</code>，所以我们 <strong>以递归的方式</strong> 将 <code>op1</code> 传入到 <code>ReverseTopo</code> 函数中；</li><li>对 <code>op1</code> 节点进行遍历，<code>op1</code> 有后继节点为 <code>op3</code> 没有被访问过，所以我们以递归的形式将 <code>op3</code> 传入到 <code>ReverseTopo</code> 函数中；</li><li>对 <code>op3</code> 节点进行遍历，分别有后继节点 <code>op4</code> 和 <code>op5</code>，均没有被访问过，所以我们都以递归的形式将 <code>op4</code> 传入到 <code>ReverseTopo</code> 函数当中；</li><li>因为 <code>op4</code> 没有后继节点，会跳出<strong>for (const auto&amp; [_, op] : next_ops)<strong>循环，所以会</strong>将本节点放入到执行队列中</strong>(<code>op4</code>)。随后该栈函数返回，<strong>返回到 4 步中以递归的形式将 <code>op5</code> 传入到 <code>ReverseTopo</code> 函数中；</strong></li><li><code>op5</code> 的后继节点进行遍历，后继节点为 <code>output</code> 节点，<code>output</code> 没有被访问过，所以再以递归的形式将 <code>op5</code> 传入到 <code>Reversetopo</code> 函数中。</li><li>最后由于 <code>output</code> 节点没有后继节点，我们将 <code>output</code> 节点 <strong>放入到执行队列中</strong>(<code>op4</code>, <code>output</code>)。随后，该栈函数返回，返回到 6 中，因为 <code>op5</code> 的后继节点已经都被访问过，所以将 <code>op5</code> 放入到执行队列中(<code>op4</code>, <code>output</code>, <code>op5</code>)。</li><li>从 6 再返回到 4 中，因为 <code>op3</code> 的所有后继都已经被访问过，所以将 <code>op3</code> <strong>放入到执行队列中</strong>(<code>op4</code>, <code>output</code>, <code>op5</code>, <code>op3</code>).</li><li>从 4 返回到 3 中，因为 <code>op1</code> 的后继节点已经都被访问过，所以将 <code>op1</code> 放入到执行队列中(<code>op4</code>, <code>output</code>, <code>op5</code>, <code>op3</code>, <code>op1</code>).</li><li>从 3 返回到 2 中，将 <code>input</code> 节点 <strong>放入到执行队列中</strong>，有(<code>op4</code>, <code>output</code>, <code>op5</code>, <code>op3</code>, <code>op1</code>, <code>input</code>).</li><li>最后需要对执行队列来一个逆序并得到最后的结果：(<code>input</code>, <code>op1</code>, <code>op3</code>, <code>op5</code>, <code>output</code>, <code>op4</code>).</li></ol><h2 id="模型的-Build-构建"><a href="#模型的-Build-构建" class="headerlink" title="模型的 Build(构建)"></a>模型的 Build(构建)</h2><p>我们在上小节中看到了 <code>model.build</code> 的调用，所以我们将在这一节中对 <code>build</code> 函数做一个补充调用。</p><h3 id="模型的状态"><a href="#模型的状态" class="headerlink" title="模型的状态"></a>模型的状态</h3><p><code>RuntimeGraph</code> 共有三个状态，表示 <strong>不同状态下的同一个模型</strong>（待初始化、待构建和构建完成），分别如下：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">enum class</span> <span class="title class_">GraphState</span> &#123;</span><br><span class="line">    NeedInit = <span class="number">-2</span>,</span><br><span class="line">    NeedBuild = <span class="number">-1</span>,</span><br><span class="line">    Complete = <span class="number">0</span>,</span><br><span class="line">  &#125;;</span><br></pre></td></tr></table></figure><p>在 <code>RuntimeGraph</code> 类中有一个变量会记录此刻模型的状态：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">GraphState graph_state_ = GraphState::NeedInit;</span><br></pre></td></tr></table></figure><p>三者的状态变换如下，<strong>依次表示待初始化，待构建和模型构建完成。</strong></p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">NeedInit --&gt; NeedBuild --&gt; Complete</span><br></pre></td></tr></table></figure><p><strong>在初始情况下</strong> 模型的状态 <code>graph_state_</code> 为 <code>NeedInit</code>，表示模型目前 <strong>待初始化</strong>。因此我们不能在此刻直接调用 <code>Build</code> 函数中的功能，<strong>而是需要在此之前先调用模型的 <code>Init</code> 函数</strong>，这一过程在下方的代码中也有体现，在初始化函数(<code>Init</code>)调用成功后会将模型的状态调整为 <code>NeedBuild</code>.</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">RuntimeGraph::Build</span><span class="params">(<span class="type">const</span> std::string&amp; input_name,</span></span></span><br><span class="line"><span class="params"><span class="function">                         <span class="type">const</span> std::string&amp; output_name)</span> </span>&#123;  </span><br><span class="line"> <span class="keyword">if</span> (graph_state_ == GraphState::Complete) &#123;</span><br><span class="line">    <span class="built_in">LOG</span>(INFO) &lt;&lt; <span class="string">&quot;Model has been built already!&quot;</span>;</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (graph_state_ == GraphState::NeedInit) &#123;</span><br><span class="line">    <span class="type">bool</span> init_graph = <span class="built_in">Init</span>();</span><br><span class="line">    <span class="built_in">LOG_IF</span>(FATAL, !init_graph) &lt;&lt; <span class="string">&quot;Init graph failed!&quot;</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="built_in">CHECK</span>(graph_state_ &gt;= GraphState::NeedBuild)</span><br></pre></td></tr></table></figure><p>以上构建(<code>Build</code>)函数中代码的目的是为了检查模型是否已经构建完成。也就是检查 <code>graph_state_ == GraphState::Complete</code>. 如果是这样的话，表示模型已经构建完成，<code>Build</code> 函数直接返回。如果模型此刻的状态是 <code>NeedInit</code>, 那我们首先 <strong>需要先对这个模型进行初始化</strong>（先调用 <code>Init</code> 函数），再进行构建。</p><p>我们再来看一下在 <code>Init</code> 函数在本节课中新增加的状态转换，可以看到在 <code>Init</code> 函数的最后，我们将模型的状态从 <code>NeedInit</code> 调整到 <code>NeedBuild</code>(需要被构建)，所以从 <code>Init</code> 函数返回之后，<code>Build</code> 函数便可以继续执行其中的代码。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">bool</span> <span class="title">RuntimeGraph::Init</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    ...</span><br><span class="line">    ...</span><br><span class="line">    graph_state_ = GraphState::NeedBuild;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br></pre></td></tr></table></figure><h3 id="继续构建图关系"><a href="#继续构建图关系" class="headerlink" title="继续构建图关系"></a>继续构建图关系</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">RuntimeGraph::Build</span><span class="params">(<span class="type">const</span> std::string&amp; input_name,</span></span></span><br><span class="line"><span class="params"><span class="function">                         <span class="type">const</span> std::string&amp; output_name)</span></span>&#123;</span><br><span class="line"> ...</span><br><span class="line"> ...</span><br><span class="line"> <span class="keyword">for</span> (<span class="type">const</span> <span class="keyword">auto</span>&amp; current_op : <span class="keyword">this</span>-&gt;operators_) &#123;</span><br><span class="line">    <span class="type">const</span> std::vector&lt;std::string&gt;&amp; output_names = current_op-&gt;output_names;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">const</span> <span class="keyword">auto</span>&amp; kOutputName : output_names) &#123;</span><br><span class="line">      <span class="keyword">if</span> (<span class="type">const</span> <span class="keyword">auto</span>&amp; output_op = <span class="keyword">this</span>-&gt;operators_maps_.<span class="built_in">find</span>(kOutputName);</span><br><span class="line">          output_op != <span class="keyword">this</span>-&gt;operators_maps_.<span class="built_in">end</span>()) &#123;</span><br><span class="line">        current_op-&gt;output_operators.<span class="built_in">insert</span>(&#123;kOutputName, output_op-&gt;second&#125;);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>我们继续往下看 <code>Build</code> 函数中的内容，这段代码主要用于 <strong>构建算子之间的联系</strong>。例如对于以下的深度学习模型结构，我们需要使用最外层的 <code>for</code> 循环遍历模型中的每一个算子 <code>current_op</code>.</p><p><img src="https://pica.zhimg.com/v2-a14969fc78f3a82ffa8a6831f1fc546a_1440w.jpg" alt="img" loading="lazy"></p><p>随后我们程序会拿到每个算子中的 <code>output_names</code>，例如对于 <code>op3</code> 来说，它的 <code>output_names</code> 包括了 <code>op4</code> 和 <code>op5</code>.</p><p>我们会对该算子(<code>op3</code>)的 <code>output_names</code> 进行遍历，并会根据 <code>output_name</code> 找到对应的后继算子并插入到 <code>op3</code> 的 <code>output_operators</code> 中。例如对于 <code>op3</code> 算子，我们会根据 <code>output_names</code> 中的 <code>&quot;op4&quot;</code> 和 <code>&quot;op5&quot;</code> 来寻找它的两个后继节点，寻找到后放入到当前计算节点 <code>current_op</code> 的后继节点列表 <code>output_operators</code> 中。</p><p>当最外层的循环结束时，<code>op1</code> 中的 <code>output_operators</code> 中存放了 <strong>它的一个后继节点</strong> <code>{&quot;op3&quot;:op3}</code>, 而 <code>op3</code> 中的 <code>output_operators</code> 会存放 <strong>它的两个后继节点</strong> <code>{&quot;op4&quot;:op4, &quot;op5&quot;:op5}</code>. 以此类推。不难看出，<code>output_operators</code> 中存放的是该节点所有后继算子节点的 <code>name</code> 到后继算子本身的映射。</p><h3 id="节点输出张量的初始化"><a href="#节点输出张量的初始化" class="headerlink" title="节点输出张量的初始化"></a>节点输出张量的初始化</h3><p>我们在 <code>Build</code> 函数中还需要 <strong>完成计算节点中输出 <a href="https://zhida.zhihu.com/search?content_id=233781267&content_type=Article&match_order=1&q=%E5%BC%A0%E9%87%8F%E7%A9%BA%E9%97%B4&zhida_source=entity">张量空间</a> 的初始化</strong>，从下图中可以看出每个节点都有一个形状不同的输出张量，用来存放该节点的计算输出。那我们为什么要在构建(<code>Build</code>)阶段就初始化这些数据，而不是在算子计算的时候再对输出空间初始化呢？这是因为 <strong>申请较大字节数的内存空间也需要一定的时间</strong>，<strong>如果放在构建阶段进行提前申请可以在【运行时】省下这段时间。</strong></p><p>另外，我们需要对算子的 <strong>输出空间</strong> 做初始化和提前申请，那么我们需要对一个算子的 <strong>输入空间初始化</strong> 吗？<strong>其实是不用的</strong>，我们借用下方的这张图，<strong>节点旁的维度表示该节点输出维度大小</strong>。</p><p>在 <code>op1</code> 节点中我们需要申请 <code>3 x 224 x 224</code> 大小的输出空间，对于它的后继节点 <code>op3</code>，我们需要申请 <code>3 x 64 x 64</code> 的内存作为输出空间，<strong>而 <code>op3</code> 的输入空间和 <code>op1</code> 的输出空间大小相同，所以 <code>op3</code> 的输入空间可以复用前驱节点(<code>op3</code>)中的输出张量空间。</strong> 同理对于 <code>add</code> 节点，我们需要为它申请 <code>3 x 32 x 32</code> 大小的输出空间，<strong>但是对于 <code>add</code> 节点的两个输入，我们可以复用前驱节点 <code>op4</code> 和 <code>op5</code> 中的输出</strong>。</p><p><img src="https://picx.zhimg.com/v2-924735125d79fdadc5f8191a00080e7f_1440w.jpg" alt="img" loading="lazy"></p><p>以上的张量空间预分配的叙述过程对应有以下的代码：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">RuntimeOperatorUtils::InitOperatorOutput</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> std::vector&lt;pnnx::Operator*&gt;&amp; pnnx_operators,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> std::vector&lt;std::shared_ptr&lt;RuntimeOperator&gt;&gt;&amp; operators)</span> </span>&#123;</span><br><span class="line">  <span class="built_in">CHECK</span>(!pnnx_operators.<span class="built_in">empty</span>() &amp;&amp; !operators.<span class="built_in">empty</span>());</span><br><span class="line">  <span class="built_in">CHECK</span>(pnnx_operators.<span class="built_in">size</span>() == operators.<span class="built_in">size</span>());</span><br></pre></td></tr></table></figure><p>我们为 <code>RuntimeOperator</code> 初始化空间时，还是需要用到 <code>PNNX</code> 中的 <code>Operator</code> 结构，所以我们首先 <strong>需要判断两个数组是否是等长的</strong>，它们具有一一对应的关系。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">RuntimeOperatorUtils::InitOperatorOutput</span><span class="params">(...)</span></span>&#123;</span><br><span class="line"> ...</span><br><span class="line"> ...</span><br><span class="line"> <span class="keyword">for</span> (<span class="type">uint32_t</span> i = <span class="number">0</span>; i &lt; pnnx_operators.<span class="built_in">size</span>(); ++i) &#123;</span><br><span class="line">    <span class="comment">// 得到pnnx原有的输出空间</span></span><br><span class="line">    <span class="type">const</span> std::vector&lt;pnnx::Operand*&gt; operands = pnnx_operators.<span class="built_in">at</span>(i)-&gt;outputs;</span><br><span class="line">    <span class="built_in">CHECK</span>(operands.<span class="built_in">size</span>() &lt;= <span class="number">1</span>) &lt;&lt; <span class="string">&quot;Only support one node one output yet!&quot;</span>;</span><br><span class="line">    <span class="keyword">if</span> (operands.<span class="built_in">empty</span>()) &#123;</span><br><span class="line">      <span class="keyword">continue</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">CHECK</span>(operands.<span class="built_in">size</span>() == <span class="number">1</span>) &lt;&lt; <span class="string">&quot;Only support one output in the KuiperInfer&quot;</span>;</span><br><span class="line"></span><br><span class="line">    pnnx::Operand* operand = operands.<span class="built_in">front</span>();</span><br><span class="line">    <span class="type">const</span> <span class="keyword">auto</span>&amp; runtime_op = operators.<span class="built_in">at</span>(i);</span><br><span class="line">    <span class="built_in">CHECK</span>(operand != <span class="literal">nullptr</span>) &lt;&lt; <span class="string">&quot;Operand output is null&quot;</span>;</span><br><span class="line">    <span class="type">const</span> std::vector&lt;<span class="type">int32_t</span>&gt;&amp; operand_shapes = operand-&gt;shape;</span><br><span class="line">    <span class="type">const</span> <span class="keyword">auto</span>&amp; output_tensors = runtime_op-&gt;output_operands;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">CHECK</span>(operand != <span class="literal">nullptr</span>) &lt;&lt; <span class="string">&quot;Operand output is null&quot;</span>;</span><br><span class="line">    <span class="type">const</span> std::vector&lt;<span class="type">int32_t</span>&gt;&amp; operand_shapes = operand-&gt;shape;</span><br></pre></td></tr></table></figure><p>在以上的代码中，我们首先使用 <code>pnnx_operators.at(i)-&gt;outputs</code> 获得第 i 个计算节点中的所有输出计算数 <code>operand</code>, <strong>我们需要根据这个 <code>pnnx</code> 计算数 <code>Operand</code> 中记录的 <code>Shape</code> 和 <code>Type</code> 信息来初始化我们 <code>runtime_op</code> 中输出 <a href="https://zhida.zhihu.com/search?content_id=233781267&content_type=Article&match_order=1&q=%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8&zhida_source=entity">数据存储</a> 的空间 <code>output_tensors = runtime_op-&gt;output_operands.</code></strong></p><p>换句话说，我们就是要根据 <code>pnnx::operand</code> 中记录的输出节点大小(<code>operand-&gt;shape</code>)来 <strong>初始化该计算节点的输出张量</strong> <code>output_tensors = runtime_op-&gt;output_operands</code> 也就是 <strong>计算节点中的具体存储空间</strong>，以及输出张量中的其他变量。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">const</span> <span class="type">int32_t</span> batch = operand_shapes.<span class="built_in">at</span>(<span class="number">0</span>);</span><br><span class="line">    <span class="built_in">CHECK</span>(batch &gt;= <span class="number">0</span>) &lt;&lt; <span class="string">&quot;Dynamic batch size is not supported!&quot;</span>;</span><br><span class="line">    <span class="built_in">CHECK</span>(operand_shapes.<span class="built_in">size</span>() == <span class="number">2</span> || operand_shapes.<span class="built_in">size</span>() == <span class="number">4</span> ||</span><br><span class="line">          operand_shapes.<span class="built_in">size</span>() == <span class="number">3</span>)</span><br><span class="line">        &lt;&lt; <span class="string">&quot;Unsupported shape sizes: &quot;</span> &lt;&lt; operand_shapes.<span class="built_in">size</span>();</span><br></pre></td></tr></table></figure><p>从上文知道，我们需要根据计算数的形状 <code>operand_shapes = operand-&gt;shape</code> 来初始化该计算节点的输出张量 <code>output_tensors</code> 中的存储空间，但是我们输出张量的维度只支持二维的、三维以及四维的，所以需要在以上代码上做 <code>check</code>.</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (!output_tensors) &#123;</span><br><span class="line">      <span class="comment">// 需要被初始化的输出张量</span></span><br><span class="line">      std::shared_ptr&lt;RuntimeOperand&gt; output_operand =</span><br><span class="line">          std::<span class="built_in">make_shared</span>&lt;RuntimeOperand&gt;();</span><br><span class="line">      <span class="comment">// 将输出操作数赋变量</span></span><br><span class="line">      output_operand-&gt;shapes = operand_shapes;</span><br><span class="line">      output_operand-&gt;type = RuntimeDataType::kTypeFloat32;</span><br><span class="line">      output_operand-&gt;name = operand-&gt;name + <span class="string">&quot;_output&quot;</span>;</span><br></pre></td></tr></table></figure><p>如果输出张量 <code>output_tensors</code> 没有被初始化，我们首先需要根据 <code>pnnx</code> 中的信息来 <strong>初始化一个保存输出张量的结构(<code>output_operand</code>)</strong>, 在这个结构中需要 <strong>保存输出张量相关的类型、名字以及维度信息</strong>。该结构的具体定义，我们已经在第二节中讲述过了，不清楚的同学可以去翻翻那节课的视频和代码。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">RuntimeOperand</span> &#123;</span><br><span class="line">  std::string name;                                     <span class="comment">/// 操作数的名称</span></span><br><span class="line">  std::vector&lt;<span class="type">int32_t</span>&gt; shapes;                          <span class="comment">/// 操作数的形状</span></span><br><span class="line">  std::vector&lt;std::shared_ptr&lt;Tensor&lt;<span class="type">float</span>&gt;&gt;&gt; datas;    <span class="comment">/// 存储操作数</span></span><br><span class="line">  RuntimeDataType type = RuntimeDataType::kTypeUnknown; <span class="comment">/// 操作数的类型</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>在上面的代码中，我们已经初始化了 <code>RuntimeOperand</code> 结构中的名字、类型和维度等信息。下面我们要去初始化结构中 <strong>存放输出数据的 <code>datas</code> 变量</strong>，它是一个张量的数组类型，数组的长度等于该计算节点的 <code>batch_size</code> 大小。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; batch; ++j) &#123;</span><br><span class="line">        <span class="keyword">if</span> (operand_shapes.<span class="built_in">size</span>() == <span class="number">4</span>) &#123;</span><br><span class="line">          sftensor output_tensor = <span class="built_in">TensorCreate</span>(</span><br><span class="line">              operand_shapes.<span class="built_in">at</span>(<span class="number">1</span>), operand_shapes.<span class="built_in">at</span>(<span class="number">2</span>), operand_shapes.<span class="built_in">at</span>(<span class="number">3</span>));</span><br><span class="line">          output_operand-&gt;datas.<span class="built_in">push_back</span>(output_tensor);</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (operand_shapes.<span class="built_in">size</span>() == <span class="number">2</span>) &#123;</span><br><span class="line">          sftensor output_tensor = <span class="built_in">TensorCreate</span>(</span><br><span class="line">              std::vector&lt;<span class="type">uint32_t</span>&gt;&#123;(<span class="type">uint32_t</span>)operand_shapes.<span class="built_in">at</span>(<span class="number">1</span>)&#125;);</span><br><span class="line">          output_operand-&gt;datas.<span class="built_in">push_back</span>(output_tensor);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          <span class="comment">// current shape is 3</span></span><br><span class="line">          sftensor output_tensor = <span class="built_in">TensorCreate</span>(std::vector&lt;<span class="type">uint32_t</span>&gt;&#123;</span><br><span class="line">              (<span class="type">uint32_t</span>)operand_shapes.<span class="built_in">at</span>(<span class="number">1</span>), (<span class="type">uint32_t</span>)operand_shapes.<span class="built_in">at</span>(<span class="number">2</span>)&#125;);</span><br><span class="line">          output_operand-&gt;datas.<span class="built_in">push_back</span>(output_tensor);</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      runtime_op-&gt;output_operands = std::<span class="built_in">move</span>(output_operand);</span><br></pre></td></tr></table></figure><p>对于一个计算算子 <code>runtime_op</code> 来说，它的输出张量数组的长度等于 <code>batch_size</code> 个，所以我们在循环中需要对 <code>batch_size</code> 个输出张量进行创建（创建的时候需要依据 <code>operand_shapes</code>, 从 <code>pnnx::operand</code> 中得到的维度）。</p><p>在创建完成后还需要放入到 <code>output_operand</code> 的 <code>datas</code> 变量中。在循环后结束后，<strong>我们会将初始化好的 <code>output_operands</code> 绑定到对应的计算节点中用于保存计算节点的输出数据。</strong> 至此，我们完成了计算节点中输出张量的初始化。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;https://cdn.jsdelivr.net/npm/aplayer@latest/dist/APlayer.min.css&quot;&gt;&lt;scrip</summary>
      
    
    
    
    <category term="KuiperInfer学习笔记" scheme="https://20020730.xyz/categories/KuiperInfer%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="C#" scheme="https://20020730.xyz/tags/C/"/>
    
    <category term="DeepLearning" scheme="https://20020730.xyz/tags/DeepLearning/"/>
    
  </entry>
  
  <entry>
    <title>Introduction</title>
    <link href="https://20020730.xyz/posts/140cab1e/"/>
    <id>https://20020730.xyz/posts/140cab1e/</id>
    <published>2025-12-03T12:00:39.000Z</published>
    <updated>2025-12-11T04:32:47.604Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer@latest/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer@latest/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting@1/dist/Meting.min.js"></script><h2 id="What-is-Mamba"><a href="#What-is-Mamba" class="headerlink" title="What is Mamba"></a>What is Mamba</h2><p>Mamba 是一种基于结构化状态空间序列模型（SSMs）的新兴架构，旨在高效捕捉序列数据中的复杂依赖性，成为 Transformer 的强大竞争对手。受经典状态空间模型启发，Mamba 融合了<a href="https://zhida.zhihu.com/search?content_id=254403907&content_type=Article&match_order=1&q=%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&zhida_source=entity">循环神经网络</a>（RNN）和<a href="https://zhida.zhihu.com/search?content_id=254403907&content_type=Article&match_order=1&q=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&zhida_source=entity">卷积神经网络</a>（CNN）的特点，通过递归或卷积操作实现计算成本与序列长度的线性或近线性扩展，显著降低计算复杂度。</p><p>具体而言，Mamba 的核心优势包括：</p><ol><li><strong>选择机制</strong>：引入简单而有效的选择机制，通过输入参数化 SSM 参数，过滤无关信息，保留必要数据。</li><li><strong>硬件感知算法</strong>：采用递归扫描而非卷积计算，优化硬件性能，在 A100 GPU 上实现高达 3 倍的加速。</li><li><strong>建模能力</strong>：在保持与 Transformer 相当的建模能力的同时，具备近线性的可扩展性，适用于复杂和长序列数据。</li></ol><p>这些特性使 Mamba 成为处理多领域任务的理想基础模型，已在计算机视觉、自然语言处理和医疗保健等领域展现出卓越性能。例如，Vim 模型在高分辨率图像特征提取中比 DeiT 快 2.8 倍，节省 86.8% 的 GPU 内存；而在语言建模任务中，改进的选择性 SSM 架构实现了 2-8 倍的加速。Mamba 的高效性和灵活性使其有望在多个研究和应用领域引发革命性变革。<br><img src="https://cdn.jsdelivr.net/gh/Wanglihan954/Picture-bed@img/img/20251129142057745.png" alt="image.png" loading="lazy"></p><h2 id="Mamba的前世今生"><a href="#Mamba的前世今生" class="headerlink" title="Mamba的前世今生"></a>Mamba的前世今生</h2><blockquote><ol><li>RNN作为第一代序列模型，奠定了序列处理的基础；</li><li>HiPPO作为理论框架，为RNN的长程依赖问题提供了数学上的最优记忆解决方案；</li><li>S4将HiPPO的连续时间理论工程化，通过离散化处理使其能够处理实际中的离散序列数据，同时引入了可训练的参数矩阵；</li><li>Mamba（S6）在S4基础上革新性地引入了动态选择性机制，使模型参数能够根据输入内容自适应调整，从而实现了接近注意力机制的内容感知能力，同时保持了线性计算复杂度。</li></ol></blockquote><h2 id="Mamba的基础知识"><a href="#Mamba的基础知识" class="headerlink" title="Mamba的基础知识"></a>Mamba的<strong>基础知识</strong></h2><p>Mamba与循环框架的循环神经网络（RNNs）、并行计算和Transformer的注意力机制以及状态空间模型（SSMs）的线性属性密切相关。因此，本节旨在介绍这三种突出架构的概述。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;https://cdn.jsdelivr.net/npm/aplayer@latest/dist/APlayer.min.css&quot;&gt;&lt;scrip</summary>
      
    
    
    
    <category term="Mamba" scheme="https://20020730.xyz/categories/Mamba/"/>
    
    
    <category term="DeepLearning" scheme="https://20020730.xyz/tags/DeepLearning/"/>
    
    <category term="python" scheme="https://20020730.xyz/tags/python/"/>
    
    <category term="Mamba" scheme="https://20020730.xyz/tags/Mamba/"/>
    
  </entry>
  
  <entry>
    <title>Lesson 5</title>
    <link href="https://20020730.xyz/posts/e52053c3/"/>
    <id>https://20020730.xyz/posts/e52053c3/</id>
    <published>2025-12-03T12:00:39.000Z</published>
    <updated>2025-12-11T04:26:32.346Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer@latest/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer@latest/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting@1/dist/Meting.min.js"></script><h1 id="第-5-课-算子和算子注册器的设计与实现"><a href="#第-5-课-算子和算子注册器的设计与实现" class="headerlink" title="第 5 课 算子和算子注册器的设计与实现"></a>第 5 课 算子和算子注册器的设计与实现</h1><p>计算节点在我们这个项目中被称之为<code>RuntimeOperator</code>, 具体的结构定义如下的代码所示：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">RuntimeOperator</span> &#123;</span><br><span class="line">  <span class="keyword">virtual</span> ~<span class="built_in">RuntimeOperator</span>();</span><br><span class="line"></span><br><span class="line">  <span class="type">bool</span> has_forward = <span class="literal">false</span>;</span><br><span class="line">  std::string name;      <span class="comment">/// 计算节点的名称</span></span><br><span class="line">  std::string type;      <span class="comment">/// 计算节点的类型</span></span><br><span class="line">  std::shared_ptr&lt;Layer&gt; layer;  <span class="comment">/// 节点对应的计算Layer</span></span><br><span class="line"></span><br><span class="line">  std::map&lt;std::string, std::shared_ptr&lt;RuntimeOperand&gt;&gt;</span><br><span class="line">      input_operands;  <span class="comment">/// 节点的输入操作数</span></span><br><span class="line">  std::shared_ptr&lt;RuntimeOperand&gt; output_operands;  <span class="comment">/// 节点的输出操作数</span></span><br><span class="line">  std::vector&lt;std::shared_ptr&lt;RuntimeOperand&gt;&gt;</span><br><span class="line">      input_operands_seq;  <span class="comment">/// 节点的输入操作数，顺序排列</span></span><br><span class="line">  std::map&lt;std::string, std::shared_ptr&lt;RuntimeOperator&gt;&gt;</span><br><span class="line">      output_operators;  <span class="comment">/// 输出节点的名字和节点对应</span></span><br><span class="line">  ...</span><br></pre></td></tr></table></figure><p>在一个计算节点(<code>RuntimeOperator</code>)中，我们记录了与该节点相关的类型、名称，以及输入输出数等信息。其中最重要的是<code>layer</code>变量，它表示与计算节点关联的算子，也就是进行具体计算的实施者。</p><p>通过访问<code>RuntimeOperator</code>的输入数(<code>input_operand</code>)，<code>layer</code>可以获取计算所需的输入张量数据，<strong>并根据<code>layer</code>各<a href="https://zhida.zhihu.com/search?content_id=233830208&content_type=Article&match_order=1&q=%E6%B4%BE%E7%94%9F%E7%B1%BB&zhida_source=entity">派生类</a>别中定义的<a href="https://zhida.zhihu.com/search?content_id=233830208&content_type=Article&match_order=1&q=%E8%AE%A1%E7%AE%97%E5%87%BD%E6%95%B0&zhida_source=entity">计算函数</a>(<code>forward</code>)对输入张量数据进行计算</strong>。计算完成后，计算结果将存储在该节点的输出数(<code>output_operand</code>)中。</p><h3 id="Layer类型的定义"><a href="#Layer类型的定义" class="headerlink" title="Layer类型的定义"></a>Layer类型的定义</h3><p>以下的代码位于<code>include/abstract/layer.hpp</code>中，<strong>它是所有算子的父类</strong>，如果要实现项目中其他的算子，都需要继承于该类作为派生类并重写其中的计算函数(<code>forward</code>)，包括我们这节课要实现的<code>ReLU</code>算子。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Layer</span> &#123;</span><br><span class="line"> <span class="keyword">public</span>:</span><br><span class="line">  <span class="function"><span class="keyword">explicit</span> <span class="title">Layer</span><span class="params">(std::string layer_name)</span> : layer_name_(std::move(layer_name)) &#123;</span>&#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">virtual</span> ~<span class="built_in">Layer</span>() = <span class="keyword">default</span>;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Layer的执行函数</span></span><br><span class="line"><span class="comment">   * @param inputs 层的输入</span></span><br><span class="line"><span class="comment">   * @param outputs 层的输出</span></span><br><span class="line"><span class="comment">   * @return 执行的状态</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">virtual</span> InferStatus <span class="title">Forward</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">      <span class="type">const</span> std::vector&lt;std::shared_ptr&lt;Tensor&lt;<span class="type">float</span>&gt;&gt;&gt;&amp; inputs,</span></span></span><br><span class="line"><span class="params"><span class="function">      std::vector&lt;std::shared_ptr&lt;Tensor&lt;<span class="type">float</span>&gt;&gt;&gt;&amp; outputs)</span></span>;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Layer的执行函数</span></span><br><span class="line"><span class="comment">   * @param current_operator 当前的operator</span></span><br><span class="line"><span class="comment">   * @return 执行的状态</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">virtual</span> InferStatus <span class="title">Forward</span><span class="params">()</span></span>;</span><br></pre></td></tr></table></figure><p>以上的代码定义了<code>Layer</code>类的<a href="https://zhida.zhihu.com/search?content_id=233830208&content_type=Article&match_order=1&q=%E6%9E%84%E9%80%A0%E5%87%BD%E6%95%B0&zhida_source=entity">构造函数</a>，它只需要一个<code>layer_name</code>变量来指定该算子的名称。我们重点关注带有参数的<code>Forward</code>方法，它是算子中定义的计算函数。</p><p>这个函数有两个参数，分别是<code>inputs</code>和<code>outputs</code>。它们是在计算过程中所需的输入和输出张量数组。<strong>每个算子的派生类都需要重写这个带参数的<code>Forward</code>方法，并在其中定义计算的具体逻辑。</strong></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Layer</span> &#123;</span><br><span class="line">    ...</span><br><span class="line">    ...</span><br><span class="line"> <span class="keyword">protected</span>:</span><br><span class="line">  std::weak_ptr&lt;RuntimeOperator&gt; runtime_operator_;</span><br><span class="line">  std::string layer_name_;  <span class="comment">/// Layer的名称</span></span><br></pre></td></tr></table></figure><p>我们可以看到，在<code>Layer</code>类中有两个<a href="https://zhida.zhihu.com/search?content_id=233830208&content_type=Article&match_order=1&q=%E6%88%90%E5%91%98%E5%8F%98%E9%87%8F&zhida_source=entity">成员变量</a>。一个是在构造函数中指定的算子名称 <code>layer_name</code>，另一个是与该算子关联的计算节点变量 <code>RuntimeOperator</code>。我们在之前回顾了 <code>RuntimeOperator</code> 的定义：</p><p><img src="https://pic3.zhimg.com/v2-433386066b3772bf92bc28a529267cca_1440w.jpg" alt="img" loading="lazy"></p><p>不难看出，<code>RuntimeOperator</code>与该节点对应的 <code>Layer</code> 相关联，而 <code>Layer</code> 也关联了它所属的 <code>RuntimeOperator</code>，因此它们之间是<a href="https://zhida.zhihu.com/search?content_id=233830208&content_type=Article&match_order=1&q=%E5%8F%8C%E5%90%91%E5%85%B3%E8%81%94&zhida_source=entity">双向关联</a>的关系。</p><p>现在我们来看一下 <code>Layer</code> 类中不带参数的 <code>Forward</code> 方法。这个方法是所有算子的<a href="https://zhida.zhihu.com/search?content_id=233830208&content_type=Article&match_order=1&q=%E7%88%B6%E7%B1%BB%E6%96%B9%E6%B3%95&zhida_source=entity">父类方法</a>，<strong>它的作用是准备输入和输出数据，并使用这些数据调用每个派生类算子中各自实现的计算过程</strong>（上文提到的带参数的 <code>Forward</code> 函数）。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">InferStatus <span class="title">Layer::Forward</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="built_in">LOG_IF</span>(FATAL, <span class="keyword">this</span>-&gt;runtime_operator_.<span class="built_in">expired</span>())</span><br><span class="line">      &lt;&lt; <span class="string">&quot;Runtime operator is expired or nullptr&quot;</span>;</span><br><span class="line">  <span class="comment">// 获取算子相关的计算节点</span></span><br><span class="line">  <span class="type">const</span> <span class="keyword">auto</span>&amp; runtime_operator = <span class="keyword">this</span>-&gt;runtime_operator_.<span class="built_in">lock</span>();</span><br><span class="line">  <span class="comment">// 准备节点layer计算所需要的输入</span></span><br><span class="line">  <span class="type">const</span> std::vector&lt;std::shared_ptr&lt;RuntimeOperand&gt;&gt;&amp; input_operand_datas = runtime_operator-&gt;input_operands_seq;</span><br><span class="line">  <span class="comment">// layer的输入</span></span><br><span class="line">  std::vector&lt;std::shared_ptr&lt;Tensor&lt;<span class="type">float</span>&gt;&gt;&gt; layer_input_datas;</span><br><span class="line">  <span class="keyword">for</span> (<span class="type">const</span> <span class="keyword">auto</span>&amp; input_operand_data : input_operand_datas) &#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">const</span> <span class="keyword">auto</span>&amp; input_data : input_operand_data-&gt;datas) &#123;</span><br><span class="line">      layer_input_datas.<span class="built_in">push_back</span>(input_data);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  ...</span><br><span class="line">  ...</span><br></pre></td></tr></table></figure><p>在<code>Layer</code>类的不带参数的<code>Forward</code>方法中，我们首先获取与该<code>Layer</code>相对应的计算节点<code>RuntimeOperator</code>。它们之间是双向关联的关系，一个算子对应一个计算节点(<code>RuntimeOperator</code>），一个计算节点对应一个算子(<code>Layer</code>)。</p><p>我们从计算节点中得到<strong>该节点对应的输入数</strong><code>input_operand_datas</code>以及<strong>该输入数存储的张量数据</strong><code>layer_input_datas</code>. 随后，我们再从计算节点中取出对应的输出数<code>output_operand_datas</code>.</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">const</span> std::shared_ptr&lt;RuntimeOperand&gt;&amp; output_operand_datas =</span><br><span class="line">      runtime_operator-&gt;output_operands;</span><br><span class="line">  InferStatus status = runtime_operator-&gt;layer-&gt;<span class="built_in">Forward</span>(</span><br><span class="line">      layer_input_datas, output_operand_datas-&gt;datas);</span><br></pre></td></tr></table></figure><p>在以上的步骤中，我们从计算节点<code>RuntimeOperator</code>中获取了相关的输入数和输出数，随后我们再使用对应的输入和输出张量<strong>去调用子类算子各自实现的，带参数的<code>Forward</code>函数</strong>。</p><p><img src="https://pic1.zhimg.com/v2-fc6adefcb609228ffa98eb0b97538610_1440w.jpg" alt="img" loading="lazy"></p><h3 id="全局的算子注册器"><a href="#全局的算子注册器" class="headerlink" title="全局的算子注册器"></a>全局的算子注册器</h3><p>在<code>KuiperInfer</code>中算子注册机制使用了<a href="https://zhida.zhihu.com/search?content_id=233830208&content_type=Article&match_order=1&q=%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F&zhida_source=entity">单例模式</a>和工厂模式。首先，在全局范围内创建一个&#x3D;&#x3D;<strong>唯一</strong>&#x3D;&#x3D;的注册表<code>registry</code>，它是一个<code>map</code>类型的对象。<strong>这个注册表的键是算子的类型，而值是算子的初始化过程。</strong></p><p><a href="https://zhida.zhihu.com/search?content_id=233830208&content_type=Article&match_order=1&q=%E5%BC%80%E5%8F%91%E8%80%85&zhida_source=entity">开发者</a>完成一个算子的开发后，需要通过特定的注册机制将算子写入全局注册表中。这可以通过在注册表中添加键值对来实现。算子的类型作为键，算子的初始化过程作为值。这样，当需要使用某个算子时，可以根据算子的类型从全局注册表中方便地获取对应的算子。</p><p>在实现上单例模式确保了只有一个全局<a href="https://zhida.zhihu.com/search?content_id=233830208&content_type=Article&match_order=6&q=%E6%B3%A8%E5%86%8C%E8%A1%A8&zhida_source=entity">注册表</a>实例，并且可以在代码的任何地方访问该注册表。<a href="https://zhida.zhihu.com/search?content_id=233830208&content_type=Article&match_order=2&q=%E5%B7%A5%E5%8E%82%E6%A8%A1%E5%BC%8F&zhida_source=entity">工厂模式</a>则负责根据算子的类型返回相应的算子实例。这种注册机制的设计使得推理框架能够感知到开发者已经实现的算子，并且能够方便地调用和使用这些算子。</p><table><thead><tr><th>算子类型</th><th>初始化过程</th></tr></thead><tbody><tr><td>Conv</td><td>ConvInstance</td></tr><tr><td>ReLU</td><td>ReLUInstance</td></tr></tbody></table><p>当所有支持的算子都被添加到注册表中后，我们可以使用<code>registry.find(layer_type)</code>来获取特定类型算子的初始化过程，并通过该初始化过程获取相应算子的实例：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LayerRegisterer</span> &#123;</span><br><span class="line"> <span class="keyword">public</span>:</span><br><span class="line">  <span class="function"><span class="keyword">typedef</span> <span class="title">ParseParameterAttrStatus</span> <span class="params">(*Creator)</span></span></span><br><span class="line"><span class="function">      <span class="params">(<span class="type">const</span> std::shared_ptr&lt;RuntimeOperator&gt; &amp;op, std::shared_ptr&lt;Layer&gt; &amp;layer)</span></span>;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">typedef</span> std::map&lt;std::string, Creator&gt; CreateRegistry;</span><br><span class="line">   <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 向注册表注册算子</span></span><br><span class="line"><span class="comment">   * @param layer_type 算子的类型</span></span><br><span class="line"><span class="comment">   * @param creator 需要注册算子的注册表</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="type">static</span> <span class="type">void</span> <span class="title">RegisterCreator</span><span class="params">(<span class="type">const</span> std::string &amp;layer_type, <span class="type">const</span> Creator &amp;creator)</span></span>;</span><br><span class="line">  ....</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><p>以上代码中的<code>creator</code>是一个<a href="https://zhida.zhihu.com/search?content_id=233830208&content_type=Article&match_order=1&q=%E5%87%BD%E6%95%B0%E6%8C%87%E9%92%88&zhida_source=entity">函数指针</a>，指向某一类算子的初始化过程，不同的算子具有不同的<a href="https://zhida.zhihu.com/search?content_id=233830208&content_type=Article&match_order=1&q=%E5%AE%9E%E4%BE%8B%E5%8C%96%E5%87%BD%E6%95%B0&zhida_source=entity">实例化函数</a>，但是都需要符合要求：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">typedef</span> <span class="title">ParseParameterAttrStatus</span> <span class="params">(*Creator)</span></span></span><br><span class="line"><span class="function">      <span class="params">(<span class="type">const</span> std::shared_ptr&lt;RuntimeOperator&gt; &amp;op,std::shared_ptr&lt;Layer&gt; &amp;layer)</span></span>;</span><br></pre></td></tr></table></figure><p>通过以上内容，我们可以观察到<strong>不同的算子实例化函数都需要接受两个参数</strong>：<code>RuntimeOperator</code>和待初始化的算子<code>layer</code>。这些函数会返回一个<code>ParseParameterAttrStatus</code>类型的状态值。</p><p>换句话说，这里的<code>Creator</code>是一个函数指针类型，用于定义某个类型算子的创建过程。当我们需要使用某个类型的算子时，可以从<code>CreateRegistry</code>类型的注册表中获取该算子的创建过程。</p><p>然后，我们将相应的<code>RuntimeOperator</code>和待初始化的<code>Layer</code>传递给创建过程，完成初始化并获得实例化后的算子。</p><h3 id="注册算子的过程"><a href="#注册算子的过程" class="headerlink" title="注册算子的过程"></a>注册算子的过程</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">LayerRegisterer::CreateRegistry&amp; <span class="title">LayerRegisterer::Registry</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="type">static</span> CreateRegistry* kRegistry = <span class="keyword">new</span> <span class="built_in">CreateRegistry</span>();</span><br><span class="line">  <span class="built_in">CHECK</span>(kRegistry != <span class="literal">nullptr</span>) &lt;&lt; <span class="string">&quot;Global layer register init failed!&quot;</span>;</span><br><span class="line">  <span class="keyword">return</span> *kRegistry;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">LayerRegisterer::RegisterCreator</span><span class="params">(<span class="type">const</span> std::string &amp;layer_type,</span></span></span><br><span class="line"><span class="params"><span class="function">                                      <span class="type">const</span> Creator &amp;creator)</span> </span>&#123;</span><br><span class="line">  <span class="built_in">CHECK</span>(creator != <span class="literal">nullptr</span>);</span><br><span class="line">  CreateRegistry &amp;registry = <span class="built_in">Registry</span>();</span><br><span class="line">  <span class="built_in">CHECK_EQ</span>(registry.<span class="built_in">count</span>(layer_type), <span class="number">0</span>)</span><br><span class="line">      &lt;&lt; <span class="string">&quot;Layer type: &quot;</span> &lt;&lt; layer_type &lt;&lt; <span class="string">&quot; has already registered!&quot;</span>;</span><br><span class="line">  registry.<span class="built_in">insert</span>(&#123;layer_type, creator&#125;);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">LayerRegisterer::CreateRegistry &amp;<span class="title">LayerRegisterer::Registry</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="type">static</span> CreateRegistry *kRegistry = <span class="keyword">new</span> <span class="built_in">CreateRegistry</span>();</span><br><span class="line">  <span class="built_in">CHECK</span>(kRegistry != <span class="literal">nullptr</span>) &lt;&lt; <span class="string">&quot;Global layer register init failed!&quot;</span>;</span><br><span class="line">  <span class="keyword">return</span> *kRegistry;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>首先，让我们来看一下<code>Registry</code>函数。这里使用了<a href="https://zhida.zhihu.com/search?content_id=233830208&content_type=Article&match_order=1&q=%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8&zhida_source=entity">线程安全</a>的懒汉式代码实现，并且利用了<code>C++11</code>标准中的**Magic Static（<a href="https://zhida.zhihu.com/search?content_id=233830208&content_type=Article&match_order=1&q=%E5%B1%80%E9%83%A8%E9%9D%99%E6%80%81%E5%8F%98%E9%87%8F&zhida_source=entity">局部静态变量</a>）**特性。在以上代码中，全局注册表<code>registry</code>变量是一个唯一的实例<code>kRegistry</code>，无论该函数被调用多少次，都会返回同一个对象。</p><p>然后，回到注册函数<code>RegisterCreator</code>。这个函数接受两个参数：算子的类型<code>layer_type</code>和<code>Creator</code>类型。正如前面所述，<code>creator</code>参数是该类算子的创建过程，它是一个函数指针。</p><p>在<code>RegistryCreator</code>函数中，首先获取全局注册表<code>registry</code>，然后检查该类型的算子是否已经被注册过。如果没有被注册过，则使用<code>.insert</code>将其插入到全局注册表。</p><h3 id="从注册器中取出算子"><a href="#从注册器中取出算子" class="headerlink" title="从注册器中取出算子"></a>从注册器中取出算子</h3><p>最后，我们来看一下如何使用注册表中已经注册过的创建过程来实例化一个算子。具体的过程如下所示：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">std::shared_ptr&lt;Layer&gt; <span class="title">LayerRegisterer::CreateLayer</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> std::shared_ptr&lt;RuntimeOperator&gt; &amp;op)</span> </span>&#123;</span><br><span class="line">  CreateRegistry &amp;registry = <span class="built_in">Registry</span>();</span><br><span class="line">  <span class="type">const</span> std::string &amp;layer_type = op-&gt;type;</span><br><span class="line">  <span class="built_in">LOG_IF</span>(FATAL, registry.<span class="built_in">count</span>(layer_type) &lt;= <span class="number">0</span>)</span><br><span class="line">      &lt;&lt; <span class="string">&quot;Can not find the layer type: &quot;</span> &lt;&lt; layer_type;</span><br><span class="line">  <span class="type">const</span> <span class="keyword">auto</span> &amp;creator = registry.<span class="built_in">find</span>(layer_type)-&gt;second;</span><br><span class="line"></span><br><span class="line">  <span class="built_in">LOG_IF</span>(FATAL, !creator) &lt;&lt; <span class="string">&quot;Layer creator is empty!&quot;</span>;</span><br><span class="line">  std::shared_ptr&lt;Layer&gt; layer;</span><br><span class="line">  <span class="type">const</span> <span class="keyword">auto</span> &amp;status = <span class="built_in">creator</span>(op, layer);</span><br><span class="line">  <span class="built_in">LOG_IF</span>(FATAL, status != ParseParameterAttrStatus::kParameterAttrParseSuccess)</span><br><span class="line">      &lt;&lt; <span class="string">&quot;Create the layer: &quot;</span> &lt;&lt; layer_type</span><br><span class="line">      &lt;&lt; <span class="string">&quot; failed, error code: &quot;</span> &lt;&lt; <span class="built_in">int</span>(status);</span><br><span class="line">  <span class="keyword">return</span> layer;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>函数<code>CreateLayer</code>用于创建算子，它接受一个名为<code>RuntimeOperator</code>的参数作为输入，该参数包含了创建算子所需的所有权重和参数信息。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">CreateRegistry &amp;registry = <span class="built_in">Registry</span>();</span><br><span class="line">  <span class="type">const</span> std::string &amp;layer_type = op-&gt;type;</span><br><span class="line">  <span class="built_in">LOG_IF</span>(FATAL, registry.<span class="built_in">count</span>(layer_type) &lt;= <span class="number">0</span>)</span><br><span class="line">      &lt;&lt; <span class="string">&quot;Can not find the layer type: &quot;</span> &lt;&lt; layer_type;</span><br></pre></td></tr></table></figure><p>在以上的代码中先获得全局注册表<code>registry</code>，再检查这个算子类型<code>layer_type</code>是否已经被注册到全局注册表中，如果已经被注册过，则获取到该算子类型对应的创建过程<code>creator</code>.</p><p>在前文中，我们说过<code>creator</code>是一个算子的创建过程函数，<strong>它的传入参数为包含所有参数和权重等信息的<code>RuntimeOperator</code>以及一个待初始化的算子<code>layer</code>.</strong> 回到<code>CreateLayer</code>函数，当<code>creator</code>函数指针被调用之后如果返回状态<code>status</code>不为<code>succcess</code>, 说明在创建过程中发生了一定的错误，算子初始化失败，需要再排查。</p><p>最后，我们再来回顾一下上面的整体过程。首先，我们定义了一个计算过程类型<code>Creator</code>和算子类型<code>Layer</code>。然后，我们定义了一个在注册表中注册算子的函数<code>RegisterCreator</code>。通过该函数，我们可以批量将算子类型和创建过程注册到全局注册表中。当需要使用某个算子时，我们可以根据算子的类型从全局注册表中获取对应的创建过程(即<code>Creator</code>类型的函数指针)。</p><p>然后，我们将创建时所需的参数和权重打包成<code>RuntimeOperator</code>类型，并传递给创建过程，类似于<code>creator(runtime_operator)</code>。这样我们就可以获得一个实例化后的算子层，整个过程就如同<code>CreateLayer</code>函数中所示。</p><h3 id="为了更方便地注册算子"><a href="#为了更方便地注册算子" class="headerlink" title="为了更方便地注册算子"></a>为了更方便地注册算子</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LayerRegistererWrapper</span> &#123;</span><br><span class="line"> <span class="keyword">public</span>:</span><br><span class="line">  <span class="built_in">LayerRegistererWrapper</span>(<span class="type">const</span> std::string &amp;layer_type, <span class="type">const</span> LayerRegisterer::Creator &amp;creator) &#123;</span><br><span class="line">    LayerRegisterer::<span class="built_in">RegisterCreator</span>(layer_type, creator);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>这个工具类只有一个构造函数，该构造函数接受算子的类型和该算子对应的创建过程作为参数。</p><p>在<code>LayerRegistererWrapper</code>类的构造函数中，我们调用<code>RegisterCreator</code>方法来完成对该算子在注册表中的注册。关于<code>RegisterCreator</code>的详细讲解，我们已经在前文提及过，不再赘述。</p><h3 id="创建第一个算子-ReLU"><a href="#创建第一个算子-ReLU" class="headerlink" title="创建第一个算子 ReLU"></a>创建第一个算子 ReLU</h3><p><img src="https://pic1.zhimg.com/v2-fc6adefcb609228ffa98eb0b97538610_1440w.jpg" alt="img" loading="lazy"></p><p><code>ReLU</code>的计算过程非常简单，有如下的定义: $ReLU(x)&#x3D;\max(0,x)$</p><p>正如前文所述，为了对输入数据进行计算，<code>ReLU</code>算子需要实现带参数的前向传播（<code>Forwards</code>）过程，实现详见<code>relu.cpp</code>.</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">InferStatus <span class="title">ReluLayer::Forward</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> std::vector&lt;std::shared_ptr&lt;Tensor&lt;<span class="type">float</span>&gt;&gt;&gt;&amp; inputs,</span></span></span><br><span class="line"><span class="params"><span class="function">    std::vector&lt;std::shared_ptr&lt;Tensor&lt;<span class="type">float</span>&gt;&gt;&gt;&amp; outputs)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (inputs.<span class="built_in">empty</span>()) &#123;</span><br><span class="line">    <span class="built_in">LOG</span>(ERROR) &lt;&lt; <span class="string">&quot;The input tensor array in the relu layer is empty&quot;</span>;</span><br><span class="line">    <span class="keyword">return</span> InferStatus::kInferFailedInputEmpty;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (inputs.<span class="built_in">size</span>() != outputs.<span class="built_in">size</span>()) &#123;</span><br><span class="line">    <span class="built_in">LOG</span>(ERROR) &lt;&lt; <span class="string">&quot;The input and output tensor array size of the relu layer do &quot;</span></span><br><span class="line">                  <span class="string">&quot;not match&quot;</span>;</span><br><span class="line">    <span class="keyword">return</span> InferStatus::kInferFailedInputOutSizeMatchError;</span><br><span class="line">  &#125;</span><br><span class="line">  ...</span><br></pre></td></tr></table></figure><p>根据公式 可以看出，<code>ReLU</code>算子不会改变输入张量的大小，也就是说输入和输出张量的维度应该是相同的。因此，<strong>上述代码首先检查输入数组是否为空，然后检查输入数组和输出数组中的元素（张量）个数是否相同，如果不满足该条件，程序返回并记录相关<a href="https://zhida.zhihu.com/search?content_id=233830208&content_type=Article&match_order=1&q=%E9%94%99%E8%AF%AF%E6%97%A5%E5%BF%97&zhida_source=entity">错误日志</a>。</strong></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">const</span> <span class="type">uint32_t</span> batch_size = inputs.<span class="built_in">size</span>();</span><br><span class="line">  <span class="keyword">for</span> (<span class="type">uint32_t</span> i = <span class="number">0</span>; i &lt; batch_size; ++i) &#123;</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">if</span> (output_data != <span class="literal">nullptr</span> &amp;&amp; !output_data-&gt;<span class="built_in">empty</span>()) &#123;</span><br><span class="line">      <span class="keyword">if</span> (input_data-&gt;<span class="built_in">shapes</span>() != output_data-&gt;<span class="built_in">shapes</span>()) &#123;</span><br><span class="line">        <span class="built_in">LOG</span>(ERROR) &lt;&lt; <span class="string">&quot;The input and output tensor shapes of the relu &quot;</span></span><br><span class="line">                      <span class="string">&quot;layer do not match &quot;</span></span><br><span class="line">                   &lt;&lt; i &lt;&lt; <span class="string">&quot; th&quot;</span>;</span><br><span class="line">        <span class="keyword">return</span> InferStatus::kInferFailedInputOutSizeMatchError;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>在以上的代码中，我们对一个批次（batch size）的输入张量数据进行了检查。我们使用<code>(output_data != nullptr &amp;&amp; !output_data-&gt;empty())</code>来检查输出张量是否为空指针，并且检查输出张量是否已经分配空间以存储计算结果。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="type">uint32_t</span> j = <span class="number">0</span>; j &lt; input-&gt;<span class="built_in">size</span>(); ++j) &#123;</span><br><span class="line">      <span class="type">float</span> value = input-&gt;<span class="built_in">index</span>(j);</span><br><span class="line">      output-&gt;<span class="built_in">index</span>(j) = value &gt; <span class="number">0.f</span> ? value : <span class="number">0.f</span>;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>在进行完以上信息检查后，我们使用一个<code>for</code>循环逐个处理一个大小为<code>batch_size</code>的输入张量数组。很明显，这个内层的<code>for</code>循环中，我们逐个读取<code>input</code>的值，并判断它与0的大小关系。如果大于0，则保留该值；否则将其置为0.</p><h3 id="ReLU算子的注册"><a href="#ReLU算子的注册" class="headerlink" title="ReLU算子的注册"></a>ReLU算子的注册</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">ParseParameterAttrStatus <span class="title">ReluLayer::GetInstance</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> std::shared_ptr&lt;RuntimeOperator&gt; &amp;op,</span></span></span><br><span class="line"><span class="params"><span class="function">    std::shared_ptr&lt;Layer&gt; &amp;relu_layer)</span> </span>&#123;</span><br><span class="line">  <span class="built_in">CHECK</span>(op != <span class="literal">nullptr</span>) &lt;&lt; <span class="string">&quot;Relu operator is nullptr&quot;</span>;</span><br><span class="line">  relu_layer = std::<span class="built_in">make_shared</span>&lt;ReluLayer&gt;();</span><br><span class="line">  <span class="keyword">return</span> ParseParameterAttrStatus::kParameterAttrParseSuccess;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">LayerRegistererWrapper <span class="title">kReluGetInstance</span><span class="params">(<span class="string">&quot;nn.ReLU&quot;</span>, ReluLayer::GetInstance)</span></span>;</span><br></pre></td></tr></table></figure><p><code>ReluLayer::GetInstance</code>是<code>ReLU</code>算子的初始化过程，该初始化函数符合之前<code>Creator</code>函数指针的参数类型、参数个数和返回值要求。该初始化函数对传入的<code>layer</code>进行初始化，并返回表示成功的状态码。</p><p><code>Creator</code>函数指针定义如下：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">typedef</span> <span class="title">ParseParameterAttrStatus</span> <span class="params">(*Creator)</span></span></span><br><span class="line"><span class="function">      <span class="params">(<span class="type">const</span> std::shared_ptr&lt;RuntimeOperator&gt; &amp;op, std::shared_ptr&lt;Layer&gt; &amp;layer)</span></span>;</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;https://cdn.jsdelivr.net/npm/aplayer@latest/dist/APlayer.min.css&quot;&gt;&lt;scrip</summary>
      
    
    
    
    <category term="KuiperInfer学习笔记" scheme="https://20020730.xyz/categories/KuiperInfer%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="C#" scheme="https://20020730.xyz/tags/C/"/>
    
    <category term="DeepLearning" scheme="https://20020730.xyz/tags/DeepLearning/"/>
    
  </entry>
  
  <entry>
    <title>Lesson 3</title>
    <link href="https://20020730.xyz/posts/c04b0c1f/"/>
    <id>https://20020730.xyz/posts/c04b0c1f/</id>
    <published>2025-12-03T12:00:39.000Z</published>
    <updated>2025-12-11T04:26:20.279Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer@latest/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer@latest/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting@1/dist/Meting.min.js"></script><h1 id="第三课-计算图的设计"><a href="#第三课-计算图的设计" class="headerlink" title="第三课 计算图的设计"></a>第三课 计算图的设计</h1><h2 id="计算图的概念"><a href="#计算图的概念" class="headerlink" title="计算图的概念"></a>计算图的概念</h2><p><code>KuiperInfer</code>使用的模型格式是<code>PNNX</code>。作为一种计算图格式，<code>PNNX</code>包含以下几个部分：</p><ol><li><code>Operator</code>:深度学习计算图中的计算节点。包括以下几个部分：<ol><li>存储输入与输出张量。</li><li>计算节点的类型与名称</li><li>参数信息（卷积核的步长，大小）</li><li>权重信息（weight，bias）</li></ol></li><li><code>Graph</code>:多个<code>Operator</code>串联成的<strong>有向无环图</strong>，规定各个<code>Operato</code>的执行流程与顺序。</li><li><code>Layer</code>:<code>Operator</code>中运行的<strong>具体执行者</strong>。</li><li><code>Tensor</code>:用于存储&#x3D;&#x3D;多维数据&#x3D;&#x3D;的数据结构，方便数据在计算节点之间传递，同时该结构也封装矩阵乘、点积等与矩阵相关的基本操作。</li></ol><h2 id="PNNX的优势"><a href="#PNNX的优势" class="headerlink" title="PNNX的优势"></a>PNNX的优势</h2><ol><li><p>使用模板匹配方法将匹配到的子图用对应的等价大算子替换。</p></li><li><p><code>Pytorch</code>中简单的算术表达式在转换为<code>PNNX</code>后，会保存表达式的整体结构，而不是拆分成许多小的加减乘除算子。</p></li><li><p><code>PNNX</code>项目中有大量图优化的技术，包括了算子融合，常量折叠和消除公共表达式等等。</p><ol><li><p>算子融合优化是一种针对深度学习神经网络的优化策略，通过将多个相邻的计算算子合并为一个算子来减少计算量和内存占用。以卷积层和批归一化层为例，我们可以把两个算子合并为一个新的算子，也就是将卷积的公式带入到批归一化层的计算公式中：<br>$$<br>Conv&#x3D;\omega* x_1+b\<br>BN&#x3D;\gamma\frac{x_2-\hat\mu}{\delta^2+\epsilon}+\beta\<br>$$<br>其中 $x_1$和 $x_2$依次是卷积和批归一化层的输入， $\omega$是卷积层的权重， b是卷积层的偏移量， $\hat\mu$和$\sigma$依次是样本的均值和方差， $\epsilon$为一个极小值。带入后有：<br>$$<br>Fused&#x3D;\gamma\frac{(\omega* x+b)-\hat\mu}{\delta^2+\epsilon}+\beta\<br>$$</p></li><li><p>常量折叠是将<strong>在编译时期间将表达式中的常量计算出来，然后将结果替换为一个等价的常量</strong>，以减少模型在运行时的计算量。</p></li><li><p>常量移除就是将计算图中不需要的常数（<strong>计算图推理的过程中未使用</strong>）节点删除，从而减少计算图的文件和加载后的资源占用大小。</p></li><li><p>公共表达式消除优化是一种针对计算图中重复计算的优化策略，<strong>它可以通过寻找并合并重复计算的计算节点，减少模型的计算量和内存占用。</strong><br>公共子表达式检测是指<strong>查找计算图中相同的子表达式</strong>，公共子表达式消除是指<strong>将这些重复计算的计算节点合并为一个新的计算节点</strong>，从而减少计算和内存开销。举个例子：<br><code>X = input(3,224,224); A = Conv(X); B = Conv(X); C = A + B</code><br>在上方的代码中，<code>Conv(X)</code>这个结果被计算了两次，公共子表达式消除可以将它优化为如下代码，这样一来就少了一次卷积的计算过程。<br><code>X = input(3, 224, 224); T = Conv(X); C = T + T</code></p></li></ol></li></ol><p>综上所述，如果在我们推理框架的底层用<code>PNNX</code>计算图，就可以吸收图优化和算子融合的结果，使得推理速度更快更高效。</p><h2 id="PNNX计算图的格式"><a href="#PNNX计算图的格式" class="headerlink" title="PNNX计算图的格式"></a>PNNX计算图的格式</h2><p><code>PNNX</code>由Graph、Operator和Operand三种结构组成，设计非常简洁。（可理解为流水线、工人和产品）</p><h3 id="PNNX中的图结构-Graph"><a href="#PNNX中的图结构-Graph" class="headerlink" title="PNNX中的图结构(Graph)"></a>PNNX中的图结构(Graph)</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Graph</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="function">Operator* <span class="title">new_operator</span><span class="params">(<span class="type">const</span> std::string&amp; type, <span class="type">const</span> std::string&amp; name)</span></span>;</span><br><span class="line">    <span class="function">Operator* <span class="title">new_operator_before</span><span class="params">(<span class="type">const</span> std::string&amp; type, <span class="type">const</span> std::string&amp; name, <span class="type">const</span> Operator* cur)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="function">Operand* <span class="title">new_operand</span><span class="params">(<span class="type">const</span> torch::jit::Value* v)</span></span>;</span><br><span class="line">    <span class="function">Operand* <span class="title">new_operand</span><span class="params">(<span class="type">const</span> std::string&amp; name)</span></span>;</span><br><span class="line">    <span class="function">Operand* <span class="title">get_operand</span><span class="params">(<span class="type">const</span> std::string&amp; name)</span></span>;</span><br><span class="line"></span><br><span class="line">    std::vector&lt;Operator*&gt; ops;</span><br><span class="line">    std::vector&lt;Operand*&gt; operands;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p><code>Graph</code>的核心作用是<strong>管理计算图中的运算符和操作数</strong>。下面我们将对这两个概念进行说明：</p><ol><li><code>Operator</code>类用来<strong>表示计算图中的运算符（算子）</strong>，比如一个模型中的<code>Convolution</code>, <code>Pooling</code>等算子；</li><li><code>Operand</code>类用来<strong>表示计算图中的操作数</strong>，即<strong>与一个运算符有关的输入和输出张量</strong>；</li><li><code>Graph</code>类的成员函数提供了方便的接口用来<strong>创建和访问操作符和操作数</strong>，以构建和遍历计算图。同时，它也是模型中<strong>运算符（算子）和操作数的集合</strong>。</li></ol><h3 id="PNNX中的运算符结构-Operator"><a href="#PNNX中的运算符结构-Operator" class="headerlink" title="PNNX中的运算符结构(Operator)"></a>PNNX中的运算符结构(Operator)</h3><p>有了上面的直观认识，我们来聊聊<code>PNNX</code>中的运算符结构。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Operator</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    std::vector&lt;Operand*&gt; inputs;</span><br><span class="line">    std::vector&lt;Operand*&gt; outputs;</span><br><span class="line"></span><br><span class="line">    std::string type;</span><br><span class="line">    std::string name;</span><br><span class="line"></span><br><span class="line">    std::vector&lt;std::string&gt; inputnames;</span><br><span class="line">    std::map&lt;std::string, Parameter&gt; params;</span><br><span class="line">    std::map&lt;std::string, Attribute&gt; attrs;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>在PNNX中，<code>Operator</code>用来表示一个算子，它由以下几个部分组成：</p><ol><li><code>inputs</code>：类型为<code>std::vector&lt;operand&gt;</code>, 表示这个算子在计算过程中所需要的<strong>输入操作数</strong><code>operand</code>；</li><li><code>outputs</code>：类型为<code>std::vector&lt;operand&gt;</code>, 表示这个算子在计算过程中得到的<strong>输出操作数</strong><code>operand</code>；</li><li><code>type</code>和<code>name</code>类型均为<code>std::string</code>, 分别表示<strong>该运算符号的类型和名称</strong>；</li><li><code>params</code>, 类型为<code>std::map</code>, 用于存放<strong>该运算符的所有参数</strong>（例如卷积运算符中的<code>params</code>中将存放<code>stride</code>, <code>padding</code>, <code>kernel size</code>等信息）；</li><li><code>attrs</code>, 类型为<code>std::map</code>, 用于存放<strong>该运算符所需要的具体权重属性</strong>（例如卷积运算符中的<code>attrs</code>中就存放着卷积的权重和偏移量，通常是一个<code>float32</code>数组）。</li></ol><h3 id="PNNX中的Attribute和Param结构"><a href="#PNNX中的Attribute和Param结构" class="headerlink" title="PNNX中的Attribute和Param结构"></a>PNNX中的Attribute和Param结构</h3><p>在PNNX中，<strong>权重数据结构(Attribute)和参数数据结构(Param)<strong>定义如下。它们通常与一个</strong>运算符</strong>(<code>Operator</code>)相关联，例如<code>Linear</code>算子的<code>in_features</code>属性和<code>weight</code>权重。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Parameter</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="comment">// 0=null 1=b 2=i 3=f 4=s 5=ai 6=af 7=as 8=others</span></span><br><span class="line">    <span class="type">int</span> type;</span><br><span class="line">    ...</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Attribute</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">Attribute</span>()</span><br><span class="line">        : <span class="built_in">type</span>(<span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">Attribute</span>(<span class="type">const</span> std::initializer_list&lt;<span class="type">int</span>&gt;&amp; shape, <span class="type">const</span> std::vector&lt;<span class="type">float</span>&gt;&amp; t);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 0=null 1=f32 2=f64 3=f16 4=i32 5=i64 6=i16 7=i8 8=u8 9=bool</span></span><br><span class="line">    <span class="type">int</span> type;</span><br><span class="line">    std::vector&lt;<span class="type">int</span>&gt; shape;</span><br><span class="line">    ...</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="PNNX中的操作数结构-Operand"><a href="#PNNX中的操作数结构-Operand" class="headerlink" title="PNNX中的操作数结构(Operand)"></a>PNNX中的操作数结构(Operand)</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Operand</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">remove_consumer</span><span class="params">(<span class="type">const</span> Operator* c)</span></span>;</span><br><span class="line">    Operator* producer;</span><br><span class="line">    std::vector&lt;Operator*&gt; consumers;</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> type;</span><br><span class="line">    std::vector&lt;<span class="type">int</span>&gt; shape;</span><br><span class="line"></span><br><span class="line">    std::string name;</span><br><span class="line">    std::map&lt;std::string, Parameter&gt; params;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>重点值得分析的是操作数结构中的<code>producer</code>和<code>customers</code>, 分别表示<strong>产生这个操作数的算子</strong>和<strong>使用这个操作数的算子</strong>。</p><p>值得注意的是&#x3D;&#x3D;产生这个操作数的算子只能有一个，而使用这个操作数的算子可以有很多个&#x3D;&#x3D;。</p><h2 id="KuiperInfer对计算图的封装"><a href="#KuiperInfer对计算图的封装" class="headerlink" title="KuiperInfer对计算图的封装"></a>KuiperInfer对计算图的封装</h2><blockquote><p>为了更好的使用底层PNNX计算图，我们会在项目中对它进行再次封装，使得PNNX更符合我们的使用需求。</p></blockquote><h3 id="UML整体结构图"><a href="#UML整体结构图" class="headerlink" title="UML整体结构图"></a>UML整体结构图</h3><p><img src="https://pic3.zhimg.com/v2-d7172bb1c064e730afb43e5aa0029aaa_1440w.jpg" alt="img" loading="lazy"></p><h3 id="对Operator的封装"><a href="#对Operator的封装" class="headerlink" title="对Operator的封装"></a>对Operator的封装</h3><p>不难从上图看出，<code>RuntimeOperator</code>是<code>KuiperInfer</code>计算图中的核心数据结构，是对<code>PNNX::Operator</code>的再次封装，它有如下的定义：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">RuntimeOperator</span> &#123;</span><br><span class="line">  <span class="keyword">virtual</span> ~<span class="built_in">RuntimeOperator</span>();</span><br><span class="line"></span><br><span class="line">  <span class="type">bool</span> has_forward = <span class="literal">false</span>;</span><br><span class="line">  std::string name;      <span class="comment">/// 计算节点的名称</span></span><br><span class="line">  std::string type;      <span class="comment">/// 计算节点的类型</span></span><br><span class="line">  std::shared_ptr&lt;Layer&gt; layer;  <span class="comment">/// 节点对应的计算Layer</span></span><br><span class="line"></span><br><span class="line">  std::vector&lt;std::string&gt; output_names;  <span class="comment">/// 节点的输出节点名称</span></span><br><span class="line">  std::shared_ptr&lt;RuntimeOperand&gt; output_operands;  <span class="comment">/// 节点的输出操作数</span></span><br><span class="line"></span><br><span class="line">  std::map&lt;std::string, std::shared_ptr&lt;RuntimeOperand&gt;&gt;</span><br><span class="line">      input_operands;  <span class="comment">/// 节点的输入操作数</span></span><br><span class="line">  std::vector&lt;std::shared_ptr&lt;RuntimeOperand&gt;&gt;</span><br><span class="line">      input_operands_seq;  <span class="comment">/// 节点的输入操作数，顺序排列</span></span><br><span class="line">  std::map&lt;std::string, std::shared_ptr&lt;RuntimeOperator&gt;&gt;</span><br><span class="line">      output_operators;  <span class="comment">/// 输出节点的名字和节点对应</span></span><br><span class="line"></span><br><span class="line">  std::map&lt;std::string, RuntimeParameter*&gt; params;  <span class="comment">/// 算子的参数信息</span></span><br><span class="line">  std::map&lt;std::string, std::shared_ptr&lt;RuntimeAttribute&gt;&gt;</span><br><span class="line">      attribute;  <span class="comment">/// 算子的属性信息，内含权重信息</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>以上这段代码定义了一个名为<code>RuntimeOperator</code>的结构体。结构体包含以下成员变量：</p><ol><li><p><code>name</code>: <strong>运算符节点的名称</strong>，可以用来区分一个唯一节点，例如 <code>Conv_1</code>, <code>Conv_2</code> 等；</p></li><li><p><code>type</code>: <strong>运算符节点的类型</strong>，例如 <code>Convolution</code>, <code>Relu</code> 等类型；</p></li><li><p><code>layer</code>: <strong>负责完成具体计算的组件</strong>，例如在 <code>Convolution Operator</code> 中，<code>layer</code> 对输入进行卷积计算，即计算其相应的卷积值；</p></li><li><p><code>input_operands</code> 和 <code>output_operands</code> 分别表示<strong>该运算符的输入和输出操作数</strong>。</p><blockquote><p>如果一个运算符(<code>RuntimeOperator</code>)的输入大小为 <code>(4, 3, 224, 224)</code>，那么在 <code>input_operands</code> 变量中，<code>datas</code> 数组的长度为 4，数组中每个元素的张量大小为 <code>(3, 224, 224)</code>；</p></blockquote></li><li><p><code>params</code> 是运算符(<code>RuntimeOperator</code>)的<strong>参数信息</strong>，包括卷积层的卷积核大小、步长等信息；</p></li><li><p><code>attribute</code> 是运算符(<code>RuntimeOperator</code>)的<strong>权重、偏移量信息</strong>，例如 <code>Matmul</code> 层或 <code>Convolution</code> 层需要的<strong>权重数据</strong>；</p></li><li><p>其他变量的含义可参考注释。</p></li></ol><h3 id="从Operator到Kuiper-RuntimeOperator"><a href="#从Operator到Kuiper-RuntimeOperator" class="headerlink" title="从Operator到Kuiper::RuntimeOperator"></a>从Operator到Kuiper::RuntimeOperator</h3><p>在这个过程中，需要先从 <code>PNNX::Operator</code> 中提取数据信息（包括我们上文提到的 <code>Operand</code> 和 <code>Operator</code> 结构），并依次填入到 <code>KuiperInfer</code> 对应的数据结构中。</p><p>相应的代码如下所示，由于篇幅原因，在课件中省略了一部分内容，完整的代码可以在配套的 <code>course3</code> 文件夹中查看。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">bool</span> <span class="title">RuntimeGraph::Init</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (<span class="keyword">this</span>-&gt;bin_path_.<span class="built_in">empty</span>() || <span class="keyword">this</span>-&gt;param_path_.<span class="built_in">empty</span>()) &#123;</span><br><span class="line">    <span class="built_in">LOG</span>(ERROR) &lt;&lt; <span class="string">&quot;The bin path or param path is empty&quot;</span>;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">this</span>-&gt;graph_ = std::<span class="built_in">make_unique</span>&lt;pnnx::Graph&gt;();</span><br><span class="line">  <span class="type">int</span> load_result = <span class="keyword">this</span>-&gt;graph_-&gt;<span class="built_in">load</span>(param_path_, bin_path_);</span><br><span class="line">  <span class="keyword">if</span> (load_result != <span class="number">0</span>) &#123;</span><br><span class="line">    <span class="built_in">LOG</span>(ERROR) &lt;&lt; <span class="string">&quot;Can not find the param path or bin path: &quot;</span> &lt;&lt; param_path_</span><br><span class="line">               &lt;&lt; <span class="string">&quot; &quot;</span> &lt;&lt; bin_path_;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  std::vector&lt;pnnx::Operator *&gt; operators = <span class="keyword">this</span>-&gt;graph_-&gt;ops;</span><br><span class="line">  <span class="keyword">for</span> (<span class="type">const</span> pnnx::Operator *op : operators) &#123;</span><br><span class="line">     std::shared_ptr&lt;RuntimeOperator&gt; runtime_operator =</span><br><span class="line">         std::<span class="built_in">make_shared</span>&lt;RuntimeOperator&gt;();</span><br><span class="line">     <span class="comment">// 初始化算子的名称</span></span><br><span class="line">     runtime_operator-&gt;name = op-&gt;name;</span><br><span class="line">     runtime_operator-&gt;type = op-&gt;type;</span><br><span class="line"></span><br><span class="line">     <span class="comment">// 初始化算子中的input</span></span><br><span class="line">     <span class="type">const</span> std::vector&lt;pnnx::Operand *&gt; &amp;inputs = op-&gt;inputs;</span><br><span class="line">     <span class="built_in">InitGraphOperatorsInput</span>(inputs, runtime_operator);</span><br><span class="line"></span><br><span class="line">     <span class="comment">// 记录输出operand中的名称</span></span><br><span class="line">     <span class="type">const</span> std::vector&lt;pnnx::Operand *&gt; &amp;outputs = op-&gt;outputs;</span><br><span class="line">     <span class="built_in">InitGraphOperatorsOutput</span>(outputs, runtime_operator);</span><br><span class="line"></span><br><span class="line">     <span class="comment">// 初始化算子中的attribute(权重)</span></span><br><span class="line">     <span class="type">const</span> std::map&lt;std::string, pnnx::Attribute&gt; &amp;attrs = op-&gt;attrs;</span><br><span class="line">     <span class="built_in">InitGraphAttrs</span>(attrs, runtime_operator);</span><br><span class="line"></span><br><span class="line">     <span class="comment">// 初始化算子中的parameter</span></span><br><span class="line">     <span class="type">const</span> std::map&lt;std::string, pnnx::Parameter&gt; &amp;params = op-&gt;params;</span><br><span class="line">     <span class="built_in">InitGraphParams</span>(params, runtime_operator);</span><br><span class="line">     <span class="keyword">this</span>-&gt;operators_.<span class="built_in">push_back</span>(runtime_operator);</span><br><span class="line">     <span class="keyword">this</span>-&gt;operators_maps_.<span class="built_in">insert</span>(&#123;runtime_operator-&gt;name, runtime_operator&#125;);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>和上文中的单元测试相同，需要先打开一个 <code>PNNX</code> 模型文件，并在返回错误时记录日志并退出。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">this</span>-&gt;graph_ = std::<span class="built_in">make_unique</span>&lt;pnnx::Graph&gt;();</span><br><span class="line">  <span class="type">int</span> load_result = <span class="keyword">this</span>-&gt;graph_-&gt;<span class="built_in">load</span>(param_path_, bin_path_);</span><br><span class="line">  <span class="keyword">if</span> (load_result != <span class="number">0</span>) &#123;</span><br><span class="line">    <span class="built_in">LOG</span>(ERROR) &lt;&lt; <span class="string">&quot;Can not find the param path or bin path: &quot;</span> &lt;&lt; param_path_</span><br><span class="line">               &lt;&lt; <span class="string">&quot; &quot;</span> &lt;&lt; bin_path_;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>在<code>for</code>循环中<strong>依次对每个运算符进行处理</strong>：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="type">const</span> pnnx::Operator *op : operators)</span><br></pre></td></tr></table></figure><p>提取<code>PNNX</code>运算符中的名字(<code>name</code>)和类型(<code>type</code>).</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">runtime_operator-&gt;name = op-&gt;name;</span><br><span class="line">runtime_operator-&gt;type = op-&gt;type;</span><br></pre></td></tr></table></figure><h3 id="提取PNNX中的操作数Operand到RuntimeOperand"><a href="#提取PNNX中的操作数Operand到RuntimeOperand" class="headerlink" title="提取PNNX中的操作数Operand到RuntimeOperand"></a>提取PNNX中的操作数Operand到RuntimeOperand</h3><p>此处的过程对应于以上代码中的<code>InitGraphOperatorsInput</code>和<code>InitGraphOperatorsOutput</code>函数。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="type">const</span> pnnx::Operator *op : operators)&#123;</span><br><span class="line">    inputs = op-&gt;inputs;</span><br><span class="line">    <span class="built_in">InitGraphOperatorsInput</span>(inputs, runtime_operator);</span><br><span class="line">    ...</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">RuntimeGraph::InitGraphOperatorsInput</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> std::vector&lt;pnnx::Operand *&gt; &amp;inputs,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> std::shared_ptr&lt;RuntimeOperator&gt; &amp;runtime_operator)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 遍历所有的输入张量</span></span><br><span class="line">  <span class="keyword">for</span> (<span class="type">const</span> pnnx::Operand *input : inputs) &#123;</span><br><span class="line">    <span class="keyword">if</span> (!input) &#123;</span><br><span class="line">      <span class="keyword">continue</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">const</span> pnnx::Operator *producer = input-&gt;producer;</span><br><span class="line">    std::shared_ptr&lt;RuntimeOperand&gt; runtime_operand =</span><br><span class="line">        std::<span class="built_in">make_shared</span>&lt;RuntimeOperand&gt;();</span><br><span class="line">    <span class="comment">// 搬运name和shape</span></span><br><span class="line">    runtime_operand-&gt;name = producer-&gt;name;</span><br><span class="line">    runtime_operand-&gt;shapes = input-&gt;shape;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">switch</span> (input-&gt;type) &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="number">1</span>: &#123;</span><br><span class="line">      <span class="comment">// 搬运类型</span></span><br><span class="line">      runtime_operand-&gt;type = RuntimeDataType::kTypeFloat32;</span><br><span class="line">      <span class="keyword">break</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">case</span> <span class="number">0</span>: &#123;</span><br><span class="line">      runtime_operand-&gt;type = RuntimeDataType::kTypeUnknown;</span><br><span class="line">      <span class="keyword">break</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">default</span>: &#123;</span><br><span class="line">      <span class="built_in">LOG</span>(FATAL) &lt;&lt; <span class="string">&quot;Unknown input operand type: &quot;</span> &lt;&lt; input-&gt;type;</span><br><span class="line">    &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    runtime_operator-&gt;input_operands.<span class="built_in">insert</span>(&#123;producer-&gt;name, runtime_operand&#125;);</span><br><span class="line">    runtime_operator-&gt;input_operands_seq.<span class="built_in">push_back</span>(runtime_operand);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>**这段代码的两个参数分别是来自 <code>PNNX</code> 中的一个运算符的所有输入操作数（<code>Operand</code>）和待初始化的 <code>RuntimeOperator</code>。**在以下的循环中：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="type">const</span> pnnx::Operand *input : inputs)</span><br></pre></td></tr></table></figure><p>我们需要依次将每个 <code>Operand</code> 中的<strong>数据信息搬运到新初始化的 <code>RuntimeOperand</code> 中</strong>，包括 <code>type</code>, <code>name</code>, <code>shapes</code> 等信息，并记录输出这个操作数(<code>Operand</code>)的运算符(<code>producer</code>)。</p><p>搬运完成后，再将数据完备的 <code>RuntimeOperand</code> 插入到待初始化的 <code>RuntimeOperator</code> 中。</p><hr><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">const</span> std::vector&lt;pnnx::Operand*&gt;&amp; outputs = op-&gt;outputs;</span><br><span class="line"><span class="built_in">InitGraphOperatorsOutput</span>(outputs, runtime_operator);</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">RuntimeGraph::InitGraphOperatorsOutput</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> std::vector&lt;pnnx::Operand *&gt; &amp;outputs,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> std::shared_ptr&lt;RuntimeOperator&gt; &amp;runtime_operator)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">for</span> (<span class="type">const</span> pnnx::Operand *output : outputs) &#123;</span><br><span class="line">    <span class="keyword">if</span> (!output) &#123;</span><br><span class="line">      <span class="keyword">continue</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">const</span> <span class="keyword">auto</span> &amp;consumers = output-&gt;consumers;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">const</span> <span class="keyword">auto</span> &amp;c : consumers) &#123;</span><br><span class="line">      runtime_operator-&gt;output_names.<span class="built_in">push_back</span>(c-&gt;name);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这段代码的两个参数分别是来自 <code>PNNX</code> 中的<strong>一个运算符的所有输出操作数</strong>（<code>Operand</code>）和待初始化的 <code>RuntimeOperator</code>.</p><p><strong>在这里，我们只需要记录操作数的消费者的名字（<code>customer.name</code>）即可</strong>。在之后的课程中，我们才会对 <code>RuntimeOperator</code> 中的输出操作数（<code>RuntimeOperand</code>）进行构建，到时再讲。</p><h3 id="提取PNNX中的权重-Attribute-到RuntimeAttribute"><a href="#提取PNNX中的权重-Attribute-到RuntimeAttribute" class="headerlink" title="提取PNNX中的权重(Attribute)到RuntimeAttribute"></a>提取PNNX中的权重(Attribute)到RuntimeAttribute</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">const</span> std::map&lt;std::string, pnnx::Attribute&gt;&amp; attrs = op-&gt;attrs;</span><br><span class="line"><span class="built_in">InitGraphAttrs</span>(attrs, runtime_operator);</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">RuntimeGraph::InitGraphAttrs</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> std::map&lt;std::string, pnnx::Attribute&gt;&amp; attrs,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> std::shared_ptr&lt;RuntimeOperator&gt;&amp; runtime_operator)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">for</span> (<span class="type">const</span> <span class="keyword">auto</span>&amp; [name, attr] : attrs) &#123;</span><br><span class="line">    <span class="keyword">switch</span> (attr.type) &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="number">1</span>: &#123;</span><br><span class="line">        std::shared_ptr&lt;RuntimeAttribute&gt; runtime_attribute =</span><br><span class="line">            std::<span class="built_in">make_shared</span>&lt;RuntimeAttribute&gt;();</span><br><span class="line">        runtime_attribute-&gt;type = RuntimeDataType::kTypeFloat32;</span><br><span class="line">        runtime_attribute-&gt;weight_data = attr.data;</span><br><span class="line">        runtime_attribute-&gt;shape = attr.shape;</span><br><span class="line">        runtime_operator-&gt;attribute.<span class="built_in">insert</span>(&#123;name, runtime_attribute&#125;);</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">default</span>: &#123;</span><br><span class="line">        <span class="built_in">LOG</span>(FATAL) &lt;&lt; <span class="string">&quot;Unknown attribute type: &quot;</span> &lt;&lt; attr.type;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这段代码的两个参数分别是来自 <code>PNNX</code> 中的<strong>一个运算符的所有权重数据结构</strong>(<code>Attribute</code>)和待初始化的<code>RuntimeOperator</code>. 在以下的循环中，</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="type">const</span> <span class="keyword">auto</span>&amp; [name, attr] : attrs)</span><br></pre></td></tr></table></figure><p>我们需要依次将 <code>Attribute</code> 中的<strong>数据信息搬运到新初始化的 <code>RuntimeAttribute</code> 中</strong>，包括 <code>type</code>, <code>weight_data</code>, <code>shapes</code> 等信息。搬运完成后，再将数据完备的 <code>RuntimeAttribute</code> 插入到待初始化的 <code>RuntimeOperator</code> 中，同时也记录这个权重的名字。</p><p>在<code>Linear</code>层中这里的<code>name</code>就是<code>weight</code>或<code>bias</code>, 对于前文测试模型中的<code>Linear</code>层，它的<code>weight shape</code>是(32, 128)，<code>weight_data</code>就是$32\times 128$个<code>float</code>数据。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;https://cdn.jsdelivr.net/npm/aplayer@latest/dist/APlayer.min.css&quot;&gt;&lt;scrip</summary>
      
    
    
    
    <category term="KuiperInfer学习笔记" scheme="https://20020730.xyz/categories/KuiperInfer%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="C#" scheme="https://20020730.xyz/tags/C/"/>
    
    <category term="DeepLearning" scheme="https://20020730.xyz/tags/DeepLearning/"/>
    
  </entry>
  
  <entry>
    <title>Lesson 2</title>
    <link href="https://20020730.xyz/posts/78f76b7a/"/>
    <id>https://20020730.xyz/posts/78f76b7a/</id>
    <published>2025-12-03T12:00:39.000Z</published>
    <updated>2025-12-11T04:26:14.110Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer@latest/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer@latest/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting@1/dist/Meting.min.js"></script><h1 id="第二课-张量（Tensor）的设计"><a href="#第二课-张量（Tensor）的设计" class="headerlink" title="第二课 张量（Tensor）的设计"></a>第二课 张量（Tensor）的设计</h1><p>一个张量类主要由以下3部分组成：</p><ol><li>数据本身，可以为<code>double</code>,<code>int</code>,<code>float</code>等等。</li><li>张量的维度形状<code>shape</code></li><li>张量类的类方法，如返回张量的宽度、高度、填充数据和张量变形等等。</li></ol><h2 id="张量类的设计"><a href="#张量类的设计" class="headerlink" title="张量类的设计"></a>张量类的设计</h2><p>本项目选择在<code>arma::fcube</code>（三维矩阵）基础上进行开发。</p><p>对于一个<code>Tensor</code>类，我们的工作主要为以下两个：</p><ol><li>提供对外接口，在<code>arma::fcube</code>基础上进行提供。</li><li>封装矩阵相关的计算功能。</li></ol><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;&gt;</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Tensor</span>&lt;<span class="type">float</span>&gt; &#123;</span><br><span class="line"> <span class="keyword">public</span>:</span><br><span class="line">  <span class="function"><span class="type">uint32_t</span> <span class="title">rows</span><span class="params">()</span> <span class="type">const</span></span>;</span><br><span class="line">  <span class="function"><span class="type">uint32_t</span> <span class="title">cols</span><span class="params">()</span> <span class="type">const</span></span>;</span><br><span class="line">  <span class="function"><span class="type">uint32_t</span> <span class="title">channels</span><span class="params">()</span> <span class="type">const</span></span>;</span><br><span class="line">  <span class="function"><span class="type">uint32_t</span> <span class="title">size</span><span class="params">()</span> <span class="type">const</span></span>;</span><br><span class="line">  <span class="function"><span class="type">void</span> <span class="title">set_data</span><span class="params">(<span class="type">const</span> arma::fcube&amp; data)</span></span>;</span><br><span class="line">  ...</span><br><span class="line">  ...</span><br><span class="line">  ...</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">  std::vector&lt;<span class="type">uint32_t</span>&gt; raw_shapes_;  <span class="comment">// 张量数据的实际尺寸大小</span></span><br><span class="line">  arma::fcube data_;                  <span class="comment">// 张量数据</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h2 id="数据的摆放顺序"><a href="#数据的摆放顺序" class="headerlink" title="数据的摆放顺序"></a>数据的摆放顺序</h2><p>两种形式：行主序和列主序。</p><h2 id="Tensor方法概述"><a href="#Tensor方法概述" class="headerlink" title="Tensor方法概述"></a>Tensor方法概述</h2><h3 id="创建张量"><a href="#创建张量" class="headerlink" title="创建张量"></a>创建张量</h3><p>首先，在所有事情开始前，我们需要创建一个张量。在创建张量——这一多维矩阵的过程中，自然而然想到我们需要用一个<code>raw_shapes</code>变量来存储张量的维度，而不同维度将决定了<code>arma::fcube</code>的具体结构。我们需要根据输入的维度信息创建相应维度的<code>arma::fcube</code>，且创建一个用于存储维度的变量。</p><ul><li>如果张量是1维的，则<code>raw_shapes</code>的长度就等于1；</li><li>如果张量是2维的，则<code>raw_shapes</code>的长度就等于2，以此类推；</li><li>在创建3维张量时，则<code>raw_shapes</code>的长度为3；</li></ul><p>&#x3D;&#x3D;值得注意的是，如果当<code>channel</code>和<code>rows</code>同时等于1时，<code>raw_shapes</code>的长度也会是1，表示此时<code>Tensor</code>是一维的；而当<code>channel</code>等于1时，<code>raw_shapes</code>的长度等于2，表示此时<code>Tensor</code>是二维的。&#x3D;&#x3D;</p><h4 id="创建1维张量"><a href="#创建1维张量" class="headerlink" title="创建1维张量"></a>创建1维张量</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Tensor&lt;<span class="type">float</span>&gt;::<span class="built_in">Tensor</span>(<span class="type">uint32_t</span> size) &#123;</span><br><span class="line">  data_ = arma::<span class="built_in">fcube</span>(<span class="number">1</span>, size, <span class="number">1</span>); <span class="comment">// 传入的参数依次是，rows cols channels</span></span><br><span class="line">  <span class="keyword">this</span>-&gt;raw_shapes_ = std::vector&lt;<span class="type">uint32_t</span>&gt;&#123;size&#125;;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="创建2维张量"><a href="#创建2维张量" class="headerlink" title="创建2维张量"></a>创建2维张量</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Tensor&lt;<span class="type">float</span>&gt;::<span class="built_in">Tensor</span>(<span class="type">uint32_t</span> rows, <span class="type">uint32_t</span> cols) &#123;</span><br><span class="line">  data_ = arma::<span class="built_in">fcube</span>(rows, cols, <span class="number">1</span>); <span class="comment">// 传入的参数依次是， rows cols channels </span></span><br><span class="line">  <span class="keyword">this</span>-&gt;raw_shapes_ = std::vector&lt;<span class="type">uint32_t</span>&gt;&#123;rows, cols&#125;;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="创建3维张量"><a href="#创建3维张量" class="headerlink" title="创建3维张量"></a>创建3维张量</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Tensor&lt;<span class="type">float</span>&gt;::<span class="built_in">Tensor</span>(<span class="type">uint32_t</span> channels, <span class="type">uint32_t</span> rows, <span class="type">uint32_t</span> cols) &#123;</span><br><span class="line">  data_ = arma::<span class="built_in">fcube</span>(rows, cols, channels);</span><br><span class="line">  <span class="keyword">if</span> (channels == <span class="number">1</span> &amp;&amp; rows == <span class="number">1</span>) &#123;</span><br><span class="line">    <span class="comment">// 当channel和rows同时等于1时，raw_shapes的长度也会是1，表示此时Tensor是一维的</span></span><br><span class="line">    <span class="keyword">this</span>-&gt;raw_shapes_ = std::vector&lt;<span class="type">uint32_t</span>&gt;&#123;cols&#125;;</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (channels == <span class="number">1</span>) &#123;</span><br><span class="line">    <span class="comment">// 当channel等于1时，raw_shapes的长度等于2，表示此时Tensor是二维的</span></span><br><span class="line">    <span class="keyword">this</span>-&gt;raw_shapes_ = std::vector&lt;<span class="type">uint32_t</span>&gt;&#123;rows, cols&#125;;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// 在创建3维张量时，则raw_shapes的长度为3，表示此时Tensor是三维的</span></span><br><span class="line">    <span class="keyword">this</span>-&gt;raw_shapes_ = std::vector&lt;<span class="type">uint32_t</span>&gt;&#123;channels, rows, cols&#125;;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="返回张量的维度信息"><a href="#返回张量的维度信息" class="headerlink" title="返回张量的维度信息"></a>返回张量的维度信息</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int32_t</span> Tensor&lt;<span class="type">float</span>&gt;::<span class="built_in">rows</span>() <span class="type">const</span> &#123;</span><br><span class="line">  <span class="built_in">CHECK</span>(!<span class="keyword">this</span>-&gt;data_.<span class="built_in">empty</span>());</span><br><span class="line">  <span class="keyword">return</span> <span class="keyword">this</span>-&gt;data_.n_rows;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">uint32_t</span> Tensor&lt;<span class="type">float</span>&gt;::<span class="built_in">cols</span>() <span class="type">const</span> &#123;</span><br><span class="line">  <span class="built_in">CHECK</span>(!<span class="keyword">this</span>-&gt;data_.<span class="built_in">empty</span>());</span><br><span class="line">  <span class="keyword">return</span> <span class="keyword">this</span>-&gt;data_.n_cols;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">uint32_t</span> Tensor&lt;<span class="type">float</span>&gt;::<span class="built_in">channels</span>() <span class="type">const</span> &#123;</span><br><span class="line">  <span class="built_in">CHECK</span>(!<span class="keyword">this</span>-&gt;data_.<span class="built_in">empty</span>());</span><br><span class="line">  <span class="keyword">return</span> <span class="keyword">this</span>-&gt;data_.n_slices;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">uint32_t</span> Tensor&lt;<span class="type">float</span>&gt;::<span class="built_in">size</span>() <span class="type">const</span> &#123;</span><br><span class="line">  <span class="built_in">CHECK</span>(!<span class="keyword">this</span>-&gt;data_.<span class="built_in">empty</span>());</span><br><span class="line">  <span class="keyword">return</span> <span class="keyword">this</span>-&gt;data_.<span class="built_in">size</span>();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="获取张量中的数据"><a href="#获取张量中的数据" class="headerlink" title="获取张量中的数据"></a>获取张量中的数据</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">const</span> arma::fmat&amp; Tensor&lt;<span class="type">float</span>&gt;::<span class="built_in">slice</span>(<span class="type">uint32_t</span> channel) <span class="type">const</span> &#123;</span><br><span class="line">  <span class="built_in">CHECK_LT</span>(channel, <span class="keyword">this</span>-&gt;<span class="built_in">channels</span>());</span><br><span class="line">  <span class="keyword">return</span> <span class="keyword">this</span>-&gt;data_.<span class="built_in">slice</span>(channel);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>以上方法用于返回<code>fcube</code>变量中的第<code>channel</code>个矩阵。换句话说，一个<code>fcube</code>作为数据的实际存储者，由多个矩阵叠加而成。当我们调用<code>slice</code>方法时，它会返回其中的第<code>channel</code>个矩阵。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">float</span> Tensor&lt;<span class="type">float</span>&gt;::<span class="built_in">at</span>(<span class="type">uint32_t</span> channel, <span class="type">uint32_t</span> row, <span class="type">uint32_t</span> col) <span class="type">const</span> &#123;</span><br><span class="line">  <span class="built_in">CHECK_LT</span>(row, <span class="keyword">this</span>-&gt;<span class="built_in">rows</span>());</span><br><span class="line">  <span class="built_in">CHECK_LT</span>(col, <span class="keyword">this</span>-&gt;<span class="built_in">cols</span>());</span><br><span class="line">  <span class="built_in">CHECK_LT</span>(channel, <span class="keyword">this</span>-&gt;<span class="built_in">channels</span>());</span><br><span class="line">  <span class="keyword">return</span> <span class="keyword">this</span>-&gt;data_.<span class="built_in">at</span>(row, col, channel);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>以上方法用于访问三维张量中第<code>(channel, row, col)</code>位置的对应数据。对于以下的<code>Tensor</code>，访问<code>(1, 1, 1)</code>位置的元素，会得到6。</p><p><img src="https://pic2.zhimg.com/v2-fe79b61594b63bfb2acbc6957bfc9f13_1440w.jpg" alt="img" loading="lazy"></p><h3 id="张量的填充"><a href="#张量的填充" class="headerlink" title="张量的填充"></a>张量的填充</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">void</span> Tensor&lt;<span class="type">float</span>&gt;::<span class="built_in">Fill</span>(<span class="type">const</span> std::vector&lt;<span class="type">float</span>&gt;&amp; values, <span class="type">bool</span> row_major) &#123;</span><br><span class="line">  <span class="built_in">CHECK</span>(!<span class="keyword">this</span>-&gt;data_.<span class="built_in">empty</span>());</span><br><span class="line">  <span class="type">const</span> <span class="type">uint32_t</span> total_elems = <span class="keyword">this</span>-&gt;data_.<span class="built_in">size</span>();</span><br><span class="line">  <span class="built_in">CHECK_EQ</span>(values.<span class="built_in">size</span>(), total_elems);</span><br><span class="line">  <span class="keyword">if</span> (row_major) &#123;</span><br><span class="line">    <span class="type">const</span> <span class="type">uint32_t</span> rows = <span class="keyword">this</span>-&gt;<span class="built_in">rows</span>();</span><br><span class="line">    <span class="type">const</span> <span class="type">uint32_t</span> cols = <span class="keyword">this</span>-&gt;<span class="built_in">cols</span>();</span><br><span class="line">    <span class="type">const</span> <span class="type">uint32_t</span> planes = rows * cols;</span><br><span class="line">    <span class="type">const</span> <span class="type">uint32_t</span> channels = <span class="keyword">this</span>-&gt;data_.n_slices;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">uint32_t</span> i = <span class="number">0</span>; i &lt; channels; ++i) &#123;</span><br><span class="line">      <span class="keyword">auto</span>&amp; channel_data = <span class="keyword">this</span>-&gt;data_.<span class="built_in">slice</span>(i);</span><br><span class="line">      <span class="type">const</span> arma::fmat&amp; <span class="type">channel_data_t</span> =</span><br><span class="line">          arma::<span class="built_in">fmat</span>(values.<span class="built_in">data</span>() + i * planes, <span class="keyword">this</span>-&gt;<span class="built_in">cols</span>(), <span class="keyword">this</span>-&gt;<span class="built_in">rows</span>());</span><br><span class="line">      channel_data = <span class="type">channel_data_t</span>.<span class="built_in">t</span>();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    std::<span class="built_in">copy</span>(values.<span class="built_in">begin</span>(), values.<span class="built_in">end</span>(), <span class="keyword">this</span>-&gt;data_.<span class="built_in">memptr</span>());</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>如果函数中的<code>row_major</code>参数为<code>true</code>，则表示按照行优先的顺序填充元素；如果该参数为<code>false</code>，则将按照列优先的顺序填充元素。</p><h3 id="对张量中的元素依次处理"><a href="#对张量中的元素依次处理" class="headerlink" title="对张量中的元素依次处理"></a>对张量中的元素依次处理</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">void</span> Tensor&lt;<span class="type">float</span>&gt;::<span class="built_in">Transform</span>(<span class="type">const</span> std::function&lt;<span class="built_in">float</span>(<span class="type">float</span>)&gt;&amp; filter) &#123;</span><br><span class="line">  <span class="built_in">CHECK</span>(!<span class="keyword">this</span>-&gt;data_.<span class="built_in">empty</span>());</span><br><span class="line">  <span class="keyword">this</span>-&gt;data_.<span class="built_in">transform</span>(filter);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>Transform</code>方法依次将张量中每个元素进行处理，处理的公式如下： $x&#x3D;transform(x)$</p><h3 id="对张量进行变形"><a href="#对张量进行变形" class="headerlink" title="对张量进行变形"></a>对张量进行变形</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">void</span> Tensor&lt;<span class="type">float</span>&gt;::<span class="built_in">Reshape</span>(<span class="type">const</span> std::vector&lt;<span class="type">uint32_t</span>&gt;&amp; shapes,</span><br><span class="line">                            <span class="type">bool</span> row_major) &#123;</span><br><span class="line">  <span class="built_in">CHECK</span>(!<span class="keyword">this</span>-&gt;data_.<span class="built_in">empty</span>());</span><br><span class="line">  <span class="built_in">CHECK</span>(!shapes.<span class="built_in">empty</span>());</span><br><span class="line">  <span class="type">const</span> <span class="type">uint32_t</span> origin_size = <span class="keyword">this</span>-&gt;<span class="built_in">size</span>();</span><br><span class="line">  <span class="type">const</span> <span class="type">uint32_t</span> current_size =</span><br><span class="line">      std::<span class="built_in">accumulate</span>(shapes.<span class="built_in">begin</span>(), shapes.<span class="built_in">end</span>(), <span class="number">1</span>, std::<span class="built_in">multiplies</span>());</span><br><span class="line">  <span class="built_in">CHECK</span>(shapes.<span class="built_in">size</span>() &lt;= <span class="number">3</span>);</span><br><span class="line">  <span class="built_in">CHECK</span>(current_size == origin_size);</span><br><span class="line"></span><br><span class="line">  std::vector&lt;<span class="type">float</span>&gt; values;</span><br><span class="line">  <span class="keyword">if</span> (row_major) &#123;</span><br><span class="line">    values = <span class="keyword">this</span>-&gt;<span class="built_in">values</span>(<span class="literal">true</span>);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (shapes.<span class="built_in">size</span>() == <span class="number">3</span>) &#123;</span><br><span class="line">    <span class="keyword">this</span>-&gt;data_.<span class="built_in">reshape</span>(<span class="number">403</span> <span class="built_in">Forbidden</span>(<span class="number">1</span>), shapes.<span class="built_in">at</span>(<span class="number">2</span>), shapes.<span class="built_in">at</span>(<span class="number">0</span>));</span><br><span class="line">    <span class="keyword">this</span>-&gt;raw_shapes_ = &#123;shapes.<span class="built_in">at</span>(<span class="number">0</span>), shapes.<span class="built_in">at</span>(<span class="number">1</span>), <span class="number">403</span> <span class="built_in">Forbidden</span>(<span class="number">2</span>)&#125;;</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (shapes.<span class="built_in">size</span>() == <span class="number">2</span>) &#123;</span><br><span class="line">    <span class="keyword">this</span>-&gt;data_.<span class="built_in">reshape</span>(shapes.<span class="built_in">at</span>(<span class="number">0</span>), <span class="number">403</span> <span class="built_in">Forbidden</span>(<span class="number">1</span>), <span class="number">1</span>);</span><br><span class="line">    <span class="keyword">this</span>-&gt;raw_shapes_ = &#123;shapes.<span class="built_in">at</span>(<span class="number">0</span>), shapes.<span class="built_in">at</span>(<span class="number">1</span>)&#125;;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">this</span>-&gt;data_.<span class="built_in">reshape</span>(<span class="number">1</span>, shapes.<span class="built_in">at</span>(<span class="number">0</span>), <span class="number">1</span>);</span><br><span class="line">    <span class="keyword">this</span>-&gt;raw_shapes_ = &#123;shapes.<span class="built_in">at</span>(<span class="number">0</span>)&#125;;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (row_major) &#123;</span><br><span class="line">    <span class="keyword">this</span>-&gt;<span class="built_in">Fill</span>(values, <span class="literal">true</span>);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这是一个复合程度更高的函数，用到了我们在之前构建好的方法。如果我们想对张量的维度进行调整，我们自然需要获取前后的形状，比如张量原先的大小是<code>(channel1, row1, col1)</code>, 再进行<code>reshape</code>之后我们将张量的大小调整为<code>(channel2, row2, col2)</code>。此外，我们还需要对内部的 data_ 的维度也进行调整，用于存放之后的数据；最后只需要将准备好的数据进行填充即可。</p><p>需要注意的是，在调整的过程中，前后的两组维度要满足以下的关系： </p><p>$(channel1\times row1 \times col1)&#x3D;(channel2 \times row2 \times col2)$</p><h3 id="张量类的辅助函数"><a href="#张量类的辅助函数" class="headerlink" title="张量类的辅助函数"></a>张量类的辅助函数</h3><h4 id="判断张量符合是否为空"><a href="#判断张量符合是否为空" class="headerlink" title="判断张量符合是否为空"></a>判断张量符合是否为空</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">bool</span> Tensor&lt;<span class="type">float</span>&gt;::<span class="built_in">empty</span>() <span class="type">const</span> &#123; <span class="keyword">return</span> <span class="keyword">this</span>-&gt;data_.<span class="built_in">empty</span>(); &#125;</span><br></pre></td></tr></table></figure><h4 id="返回张量数据存储区域的起始地址"><a href="#返回张量数据存储区域的起始地址" class="headerlink" title="返回张量数据存储区域的起始地址"></a>返回张量数据存储区域的起始地址</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">const</span> <span class="type">float</span>* Tensor&lt;<span class="type">float</span>&gt;::<span class="built_in">raw_ptr</span>() <span class="type">const</span> &#123;</span><br><span class="line">  <span class="built_in">CHECK</span>(!<span class="keyword">this</span>-&gt;data_.<span class="built_in">empty</span>());</span><br><span class="line">  <span class="keyword">return</span> <span class="keyword">this</span>-&gt;data_.<span class="built_in">memptr</span>();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上文说到，张量类中数据存储由三维矩阵类(<code>fcube</code>)负责，所以在<code>raw_ptr</code>的目的就是返回数据存储的起始位置。</p><h4 id="返回张量的shape"><a href="#返回张量的shape" class="headerlink" title="返回张量的shape"></a>返回张量的shape</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">const</span> std::vector&lt;<span class="type">uint32_t</span>&gt;&amp; Tensor&lt;<span class="type">float</span>&gt;::<span class="built_in">raw_shapes</span>() <span class="type">const</span> &#123;</span><br><span class="line">  <span class="built_in">CHECK</span>(!<span class="keyword">this</span>-&gt;raw_shapes_.<span class="built_in">empty</span>());</span><br><span class="line">  <span class="built_in">CHECK_LE</span>(<span class="keyword">this</span>-&gt;raw_shapes_.<span class="built_in">size</span>(), <span class="number">3</span>);</span><br><span class="line">  <span class="built_in">CHECK_GE</span>(<span class="keyword">this</span>-&gt;raw_shapes_.<span class="built_in">size</span>(), <span class="number">1</span>);</span><br><span class="line">  <span class="keyword">return</span> <span class="keyword">this</span>-&gt;raw_shapes_;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>返回张量的三维形状<code>[channels, rows, cols]</code>.</p><ul><li>如果 <code>channels = 1</code> 并且 <code>rows = 1</code>，则 <code>raw_shapes</code> 返回一维形状 <code>[cols]</code>，张量是一个一维张量。</li><li>如果 <code>channels = 1</code>，则 <code>raw_shapes</code> 返回二维形状 <code>[rows, cols]</code>，张量是一个二维张量。</li><li>否则表明该<code>Tensor</code>就是一个三维张量，<code>raw_shapes</code> 返回三维形状 <code>[channels, rows, cols]</code>.</li></ul><h3 id="练习"><a href="#练习" class="headerlink" title="练习"></a>练习</h3><h4 id="Flatten"><a href="#Flatten" class="headerlink" title="Flatten"></a>Flatten</h4><p>编写<code>Tensor::Flatten</code>方法，将多维展开成一维。</p><p><img src="https://i-blog.csdnimg.cn/blog_migrate/8897f8d941fd83fb1b50430ffa65adb6.png" alt="在这里插入图片描述" loading="lazy"></p><p>观察函数声明和单元测试</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">void</span> Tensor&lt;<span class="type">float</span>&gt;::<span class="built_in">Flatten</span>(<span class="type">bool</span> row_major) &#123;</span><br><span class="line">  <span class="built_in">CHECK</span>(!<span class="keyword">this</span>-&gt;data_.<span class="built_in">empty</span>());</span><br><span class="line">  <span class="comment">// 请补充代码</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="built_in">TEST</span>(test_homework, homework1_flatten1) &#123;</span><br><span class="line">  <span class="keyword">using</span> <span class="keyword">namespace</span> kuiper_infer;</span><br><span class="line">  <span class="function">Tensor&lt;<span class="type">float</span>&gt; <span class="title">f1</span><span class="params">(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span></span>;</span><br><span class="line">  f<span class="number">1.F</span>latten(<span class="literal">true</span>);</span><br><span class="line">  <span class="built_in">ASSERT_EQ</span>(f<span class="number">1.</span><span class="built_in">raw_shapes</span>().<span class="built_in">size</span>(), <span class="number">1</span>);</span><br><span class="line">  <span class="built_in">ASSERT_EQ</span>(f<span class="number">1.</span><span class="built_in">raw_shapes</span>().<span class="built_in">at</span>(<span class="number">0</span>), <span class="number">24</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="built_in">TEST</span>(test_homework, homework1_flatten2) &#123;</span><br><span class="line">  <span class="keyword">using</span> <span class="keyword">namespace</span> kuiper_infer;</span><br><span class="line">  <span class="function">Tensor&lt;<span class="type">float</span>&gt; <span class="title">f1</span><span class="params">(<span class="number">12</span>, <span class="number">24</span>)</span></span>;</span><br><span class="line">  f<span class="number">1.F</span>latten(<span class="literal">true</span>);</span><br><span class="line">  <span class="built_in">ASSERT_EQ</span>(f<span class="number">1.</span><span class="built_in">raw_shapes</span>().<span class="built_in">size</span>(), <span class="number">1</span>);</span><br><span class="line">  <span class="built_in">ASSERT_EQ</span>(f<span class="number">1.</span><span class="built_in">raw_shapes</span>().<span class="built_in">at</span>(<span class="number">0</span>), <span class="number">24</span> * <span class="number">12</span>);</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>方法实现，调用Reshape即可</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">void</span> Tensor&lt;<span class="type">float</span>&gt;::<span class="built_in">Flatten</span>(<span class="type">bool</span> row_major) &#123;</span><br><span class="line">  <span class="built_in">CHECK</span>(!<span class="keyword">this</span>-&gt;data_.<span class="built_in">empty</span>());</span><br><span class="line">  <span class="comment">// 请补充代码</span></span><br><span class="line">  std::vector&lt;<span class="type">uint32_t</span>&gt; new_shapes = std::vector&lt;<span class="type">uint32_t</span>&gt;&#123; <span class="keyword">this</span>-&gt;<span class="built_in">size</span>() &#125;;</span><br><span class="line">  <span class="keyword">this</span>-&gt;<span class="built_in">Reshape</span>(new_shapes, row_major);</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="Padding"><a href="#Padding" class="headerlink" title="Padding"></a>Padding</h4><p>编写<code>Tensor::Padding</code>函数，在张量周围做填充</p><p><img src="https://i-blog.csdnimg.cn/blog_migrate/54394bdf2b59cd9879bcf70ad7696f40.png" alt="在这里插入图片描述" loading="lazy"></p><p>观察函数声明和单元测试</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 填充张量</span></span><br><span class="line"><span class="comment"> * @param pads 填充张量的尺寸</span></span><br><span class="line"><span class="comment"> * @param padding_value 填充张量</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="type">void</span> Tensor&lt;<span class="type">float</span>&gt;::<span class="built_in">Padding</span>(<span class="type">const</span> std::vector&lt;<span class="type">uint32_t</span>&gt;&amp; pads,</span><br><span class="line">                            <span class="type">float</span> padding_value) &#123;</span><br><span class="line">  <span class="built_in">CHECK</span>(!<span class="keyword">this</span>-&gt;data_.<span class="built_in">empty</span>());</span><br><span class="line">  <span class="built_in">CHECK_EQ</span>(pads.<span class="built_in">size</span>(), <span class="number">4</span>);</span><br><span class="line">  <span class="comment">// 四周填充的维度</span></span><br><span class="line">  <span class="type">uint32_t</span> pad_rows1 = pads.<span class="built_in">at</span>(<span class="number">0</span>);  <span class="comment">// up</span></span><br><span class="line">  <span class="type">uint32_t</span> pad_rows2 = pads.<span class="built_in">at</span>(<span class="number">1</span>);  <span class="comment">// bottom</span></span><br><span class="line">  <span class="type">uint32_t</span> pad_cols1 = pads.<span class="built_in">at</span>(<span class="number">2</span>);  <span class="comment">// left</span></span><br><span class="line">  <span class="type">uint32_t</span> pad_cols2 = pads.<span class="built_in">at</span>(<span class="number">3</span>);  <span class="comment">// right</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// 请补充代码</span></span><br><span class="line">    </span><br><span class="line">&#125;</span><br><span class="line"><span class="built_in">TEST</span>(test_homework, homework2_padding1) &#123;</span><br><span class="line">  <span class="keyword">using</span> <span class="keyword">namespace</span> kuiper_infer;</span><br><span class="line">  <span class="function">Tensor&lt;<span class="type">float</span>&gt; <span class="title">tensor</span><span class="params">(<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>)</span></span>;</span><br><span class="line">  <span class="built_in">ASSERT_EQ</span>(tensor.<span class="built_in">channels</span>(), <span class="number">3</span>);</span><br><span class="line">  <span class="built_in">ASSERT_EQ</span>(tensor.<span class="built_in">rows</span>(), <span class="number">4</span>);</span><br><span class="line">  <span class="built_in">ASSERT_EQ</span>(tensor.<span class="built_in">cols</span>(), <span class="number">5</span>);</span><br><span class="line"></span><br><span class="line">  tensor.<span class="built_in">Fill</span>(<span class="number">1.f</span>);</span><br><span class="line">  tensor.<span class="built_in">Padding</span>(&#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>&#125;, <span class="number">0</span>);</span><br><span class="line">  <span class="built_in">ASSERT_EQ</span>(tensor.<span class="built_in">rows</span>(), <span class="number">7</span>);</span><br><span class="line">  <span class="built_in">ASSERT_EQ</span>(tensor.<span class="built_in">cols</span>(), <span class="number">12</span>);</span><br><span class="line"></span><br><span class="line">  <span class="type">int</span> index = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">for</span> (<span class="type">int</span> c = <span class="number">0</span>; c &lt; tensor.<span class="built_in">channels</span>(); ++c) &#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> r = <span class="number">0</span>; r &lt; tensor.<span class="built_in">rows</span>(); ++r) &#123;</span><br><span class="line">      <span class="keyword">for</span> (<span class="type">int</span> c_ = <span class="number">0</span>; c_ &lt; tensor.<span class="built_in">cols</span>(); ++c_) &#123;</span><br><span class="line">        <span class="keyword">if</span> ((r &gt;= <span class="number">2</span> &amp;&amp; r &lt;= <span class="number">4</span>) &amp;&amp; (c_ &gt;= <span class="number">3</span> &amp;&amp; c_ &lt;= <span class="number">7</span>)) &#123;</span><br><span class="line">          <span class="built_in">ASSERT_EQ</span>(tensor.<span class="built_in">at</span>(c, r, c_), <span class="number">1.f</span>) &lt;&lt; c &lt;&lt; <span class="string">&quot; &quot;</span></span><br><span class="line">                                              &lt;&lt; <span class="string">&quot; &quot;</span> &lt;&lt; r &lt;&lt; <span class="string">&quot; &quot;</span> &lt;&lt; c_;</span><br><span class="line">        &#125;</span><br><span class="line">        index += <span class="number">1</span>;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="built_in">TEST</span>(test_homework, homework2_padding2) &#123;</span><br><span class="line">  <span class="keyword">using</span> <span class="keyword">namespace</span> kuiper_infer;</span><br><span class="line">  <span class="function">ftensor <span class="title">tensor</span><span class="params">(<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>)</span></span>;</span><br><span class="line">  <span class="built_in">ASSERT_EQ</span>(tensor.<span class="built_in">channels</span>(), <span class="number">3</span>);</span><br><span class="line">  <span class="built_in">ASSERT_EQ</span>(tensor.<span class="built_in">rows</span>(), <span class="number">4</span>);</span><br><span class="line">  <span class="built_in">ASSERT_EQ</span>(tensor.<span class="built_in">cols</span>(), <span class="number">5</span>);</span><br><span class="line"></span><br><span class="line">  tensor.<span class="built_in">Fill</span>(<span class="number">1.f</span>);</span><br><span class="line">  tensor.<span class="built_in">Padding</span>(&#123;<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>&#125;, <span class="number">3.14f</span>);</span><br><span class="line">  <span class="built_in">ASSERT_EQ</span>(tensor.<span class="built_in">rows</span>(), <span class="number">8</span>);</span><br><span class="line">  <span class="built_in">ASSERT_EQ</span>(tensor.<span class="built_in">cols</span>(), <span class="number">9</span>);</span><br><span class="line"></span><br><span class="line">  <span class="type">int</span> index = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">for</span> (<span class="type">int</span> c = <span class="number">0</span>; c &lt; tensor.<span class="built_in">channels</span>(); ++c) &#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> r = <span class="number">0</span>; r &lt; tensor.<span class="built_in">rows</span>(); ++r) &#123;</span><br><span class="line">      <span class="keyword">for</span> (<span class="type">int</span> c_ = <span class="number">0</span>; c_ &lt; tensor.<span class="built_in">cols</span>(); ++c_) &#123;</span><br><span class="line">        <span class="keyword">if</span> (c_ &lt;= <span class="number">1</span> || r &lt;= <span class="number">1</span>) &#123;</span><br><span class="line">          <span class="built_in">ASSERT_EQ</span>(tensor.<span class="built_in">at</span>(c, r, c_), <span class="number">3.14f</span>);</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (c &gt;= tensor.<span class="built_in">cols</span>() - <span class="number">1</span> || r &gt;= tensor.<span class="built_in">rows</span>() - <span class="number">1</span>) &#123;</span><br><span class="line">          <span class="built_in">ASSERT_EQ</span>(tensor.<span class="built_in">at</span>(c, r, c_), <span class="number">3.14f</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> ((r &gt;= <span class="number">2</span> &amp;&amp; r &lt;= <span class="number">5</span>) &amp;&amp; (c_ &gt;= <span class="number">2</span> &amp;&amp; c_ &lt;= <span class="number">6</span>)) &#123;</span><br><span class="line">          <span class="built_in">ASSERT_EQ</span>(tensor.<span class="built_in">at</span>(c, r, c_), <span class="number">1.f</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        index += <span class="number">1</span>;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>思路是先把保存原始数据，接着把<code>data_</code>变成对应填充后的<code>shape</code>，之后创建全新数组<code>pad_values</code>并全部填充上<code>padding_value</code>，然后把<code>pad_values</code>对应位置填上原始数据，最后调用<code>Fill</code>方法将<code>pad_values</code>填充入<code>data_</code>。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">void</span> Tensor&lt;<span class="type">float</span>&gt;::<span class="built_in">Padding</span>(<span class="type">const</span> std::vector&lt;<span class="type">uint32_t</span>&gt;&amp; pads,</span><br><span class="line">                            <span class="type">float</span> padding_value) &#123;</span><br><span class="line">  <span class="built_in">CHECK</span>(!<span class="keyword">this</span>-&gt;data_.<span class="built_in">empty</span>());</span><br><span class="line">  <span class="built_in">CHECK_EQ</span>(pads.<span class="built_in">size</span>(), <span class="number">4</span>);</span><br><span class="line">  <span class="comment">// 四周填充的维度</span></span><br><span class="line">  <span class="type">uint32_t</span> pad_rows1 = pads.<span class="built_in">at</span>(<span class="number">0</span>);  <span class="comment">// up</span></span><br><span class="line">  <span class="type">uint32_t</span> pad_rows2 = pads.<span class="built_in">at</span>(<span class="number">1</span>);  <span class="comment">// bottom</span></span><br><span class="line">  <span class="type">uint32_t</span> pad_cols1 = pads.<span class="built_in">at</span>(<span class="number">2</span>);  <span class="comment">// left</span></span><br><span class="line">  <span class="type">uint32_t</span> pad_cols2 = pads.<span class="built_in">at</span>(<span class="number">3</span>);  <span class="comment">// right</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// 请补充代码</span></span><br><span class="line">  <span class="comment">// params needed</span></span><br><span class="line">  <span class="type">uint32_t</span> ori_rows = <span class="keyword">this</span>-&gt;<span class="built_in">rows</span>();</span><br><span class="line">  <span class="type">uint32_t</span> ori_cols = <span class="keyword">this</span>-&gt;<span class="built_in">cols</span>();</span><br><span class="line">  <span class="type">uint32_t</span> new_rows = <span class="keyword">this</span>-&gt;<span class="built_in">rows</span>() + pad_rows1 + pad_rows2;</span><br><span class="line">  <span class="type">uint32_t</span> new_cols = <span class="keyword">this</span>-&gt;<span class="built_in">cols</span>() + pad_cols1 + pad_cols2;</span><br><span class="line">  <span class="type">uint32_t</span> channels = <span class="keyword">this</span>-&gt;<span class="built_in">channels</span>();</span><br><span class="line">  <span class="type">const</span> std::vector&lt;<span class="type">float</span>&gt;&amp; ori_values = <span class="keyword">this</span>-&gt;<span class="built_in">values</span>();</span><br><span class="line"></span><br><span class="line">  <span class="comment">// new data members</span></span><br><span class="line">  <span class="keyword">this</span>-&gt;data_ = arma::<span class="built_in">fcube</span>(new_rows, new_cols, channels);</span><br><span class="line">  <span class="keyword">this</span>-&gt;raw_shapes_ = std::vector&lt;<span class="type">uint32_t</span>&gt;&#123; channels, new_rows, new_cols &#125;;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// fill pad values, row_major</span></span><br><span class="line">  <span class="built_in">CHECK_EQ</span>(<span class="keyword">this</span>-&gt;<span class="built_in">size</span>(), new_rows * new_cols * channels);</span><br><span class="line">  std::vector&lt;<span class="type">float</span>&gt; pad_values = std::<span class="built_in">vector</span>&lt;<span class="type">float</span>&gt;(<span class="keyword">this</span>-&gt;<span class="built_in">size</span>());</span><br><span class="line">  std::<span class="built_in">fill</span>(pad_values.<span class="built_in">begin</span>(), pad_values.<span class="built_in">end</span>(), padding_value);</span><br><span class="line">  </span><br><span class="line">  <span class="type">uint32_t</span> ori_channelsize = ori_rows * ori_cols;</span><br><span class="line">  <span class="type">uint32_t</span> pad_channelsize = new_cols * new_rows;</span><br><span class="line">  <span class="keyword">for</span> (<span class="type">uint32_t</span> channel = <span class="number">0</span>; channel &lt; channels; ++channel) &#123;</span><br><span class="line">      <span class="keyword">for</span> (<span class="type">uint32_t</span> row = <span class="number">0</span>; row &lt; ori_rows; ++row) &#123;</span><br><span class="line">          <span class="type">uint32_t</span> pad_row = row + pad_rows1;</span><br><span class="line">          std::<span class="built_in">copy</span>(ori_values.<span class="built_in">begin</span>() + channel * ori_channelsize + row * ori_cols,</span><br><span class="line">              ori_values.<span class="built_in">begin</span>() + channel * ori_channelsize + (row + <span class="number">1</span>) * ori_cols,</span><br><span class="line">              pad_values.<span class="built_in">begin</span>() + channel * pad_channelsize + pad_row * new_cols + pad_cols1);</span><br><span class="line">      &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="built_in">CHECK_EQ</span>(<span class="keyword">this</span>-&gt;<span class="built_in">size</span>(), pad_values.<span class="built_in">size</span>());</span><br><span class="line">  <span class="keyword">this</span>-&gt;<span class="built_in">Fill</span>(pad_values);</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;https://cdn.jsdelivr.net/npm/aplayer@latest/dist/APlayer.min.css&quot;&gt;&lt;scrip</summary>
      
    
    
    
    <category term="KuiperInfer学习笔记" scheme="https://20020730.xyz/categories/KuiperInfer%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="C#" scheme="https://20020730.xyz/tags/C/"/>
    
    <category term="DeepLearning" scheme="https://20020730.xyz/tags/DeepLearning/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="https://20020730.xyz/posts/4a17b156/"/>
    <id>https://20020730.xyz/posts/4a17b156/</id>
    <published>2025-11-29T11:39:29.448Z</published>
    <updated>2025-12-11T04:29:28.829Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer@latest/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer@latest/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting@1/dist/Meting.min.js"></script><p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;https://cdn.jsdelivr.net/npm/aplayer@latest/dist/APlayer.min.css&quot;&gt;&lt;scrip</summary>
      
    
    
    
    
  </entry>
  
</feed>
