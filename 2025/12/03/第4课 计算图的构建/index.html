<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="theme-color" content="#0078E7"><meta name="author" content="宁翰"><meta name="copyright" content="宁翰"><meta name="generator" content="Hexo 8.1.1"><meta name="theme" content="hexo-theme-yun"><title>第4课 | 宁翰のHEXO</title><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@900&amp;display=swap" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/star-markdown-css@0.4.1/dist/yun/yun-markdown.min.css"><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/prism-theme-vars/base.css"><script src="https://fastly.jsdelivr.net/npm/scrollreveal/dist/scrollreveal.min.js" defer></script><script>function initScrollReveal() {
  [".post-card",".markdown-body img"].forEach((target)=> {
    ScrollReveal().reveal(target);
  })
}
document.addEventListener("DOMContentLoaded", initScrollReveal);
document.addEventListener("pjax:success", initScrollReveal);
</script><link rel="icon" type="image/png" href="/favicon.ico"><link rel="mask-icon" href="/favicon.ico" color="#0078E7"><link rel="preload" href="/css/hexo-theme-yun.css" as="style"><link rel="prefetch" href="/js/sidebar.js" as="script"><link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin><link rel="preconnect" href="https://fastly.jsdelivr.net/npm/" crossorigin><script id="yun-config">
    window.Yun = {}
    window.CONFIG = {"hostname":"20020730.xyz","root":"/","title":"让梦成为可能","version":"1.10.11","mode":"auto","copycode":true,"page":{"isPost":true},"i18n":{"placeholder":"搜索...","empty":"找不到您查询的内容: ${query}","hits":"找到 ${hits} 条结果","hits_time":"找到 ${hits} 条结果（用时 ${time} 毫秒）"},"anonymous_image":"https://cdn.yunyoujun.cn/img/avatar/none.jpg","say":{"api":"https://el-bot-api.vercel.app/api/words/young"},"fireworks":{"colors":null},"vendors":{"host":"https://fastly.jsdelivr.net/npm/","darken":"https://fastly.jsdelivr.net/npm/darken@1.5.0"}};
  </script><link rel="stylesheet" href="/css/hexo-theme-yun.css"><script src="/js/hexo-theme-yun.js" type="module"></script><meta name="description" content="第四课 计算图的构建拓扑排序 对于一个有向无环图，拓扑排序总能够找到一个 节点序列，在这个序列中，每个节点的前驱节点都能排在这个节点的前面。什么是 前驱节点 呢，也就是对于 有向图中任意一条边的起点，我们可以认为它是 终点节点 的前驱节点。 对于上图例子中的深度学习模型，conv1 是 conv2 的前驱节点，conv1 也是 conv3 的前驱节点，所以在计算执行节点序列时，conv1 必须出现">
<meta property="og:type" content="article">
<meta property="og:title" content="第4课">
<meta property="og:url" content="https://20020730.xyz/2025/12/03/%E7%AC%AC4%E8%AF%BE%20%E8%AE%A1%E7%AE%97%E5%9B%BE%E7%9A%84%E6%9E%84%E5%BB%BA/index.html">
<meta property="og:site_name" content="宁翰のHEXO">
<meta property="og:description" content="第四课 计算图的构建拓扑排序 对于一个有向无环图，拓扑排序总能够找到一个 节点序列，在这个序列中，每个节点的前驱节点都能排在这个节点的前面。什么是 前驱节点 呢，也就是对于 有向图中任意一条边的起点，我们可以认为它是 终点节点 的前驱节点。 对于上图例子中的深度学习模型，conv1 是 conv2 的前驱节点，conv1 也是 conv3 的前驱节点，所以在计算执行节点序列时，conv1 必须出现">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://pica.zhimg.com/v2-37ed6844956354c8bad07d70723a1794_1440w.jpg">
<meta property="og:image" content="https://pic4.zhimg.com/v2-e20ad84bd6a3b0c9198388b30766756f_1440w.jpg">
<meta property="og:image" content="https://pica.zhimg.com/v2-a14969fc78f3a82ffa8a6831f1fc546a_1440w.jpg">
<meta property="og:image" content="https://picx.zhimg.com/v2-924735125d79fdadc5f8191a00080e7f_1440w.jpg">
<meta property="article:published_time" content="2025-12-03T12:00:39.000Z">
<meta property="article:modified_time" content="2025-12-09T14:47:36.871Z">
<meta property="article:author" content="宁翰">
<meta property="article:tag" content="Dream it possible">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://pica.zhimg.com/v2-37ed6844956354c8bad07d70723a1794_1440w.jpg"><script>(function() {
  if (CONFIG.mode !== 'auto') return
  const prefersDark = window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches
  const setting = localStorage.getItem('darken-mode') || 'auto'
  if (setting === 'dark' || (prefersDark && setting !== 'light'))
    document.documentElement.classList.toggle('dark', true)
})()</script></head><body><script src="https://code.iconify.design/2/2.1.1/iconify.min.js"></script><script>// Define global variable
IconifyProviders = {
  // Empty prefix: overwrite default API provider configuration
  '': {
    // Use custom API first, use Iconify public API as backup
    resources: [
        'https://api.iconify.design',
    ],
    // Wait for 1 second before switching API hosts
    rotate: 1000,
  },
};</script><script defer src="https://fastly.jsdelivr.net/npm/animejs@latest"></script><script defer src="/js/ui/fireworks.js" type="module"></script><canvas class="fireworks"></canvas><div class="container"><a class="sidebar-toggle hty-icon-button" id="menu-btn"><div class="hamburger hamburger--spin" type="button"><span class="hamburger-box"><span class="hamburger-inner"></span></span></div></a><div class="sidebar-toggle sidebar-overlay"></div><aside class="sidebar"><script src="/js/sidebar.js" type="module"></script><ul class="sidebar-nav"><li class="sidebar-nav-item sidebar-nav-toc hty-icon-button sidebar-nav-active" data-target="post-toc-wrap" title="文章目录"><span class="icon iconify" data-icon="ri:list-ordered"></span></li><li class="sidebar-nav-item sidebar-nav-overview hty-icon-button" data-target="site-overview-wrap" title="站点概览"><span class="icon iconify" data-icon="ri:passport-line"></span></li></ul><div class="sidebar-panel" id="site-overview-wrap"><div class="site-info fix-top"><a class="site-author-avatar" href="/about/" title="宁翰"><img width="96" loading="lazy" src="/images/funingna.png" alt="宁翰"><span class="site-author-status" title="四季花开"></span></a><div class="site-author-name"><a href="/about/">宁翰</a></div><span class="site-name">宁翰のHEXO</span><sub class="site-subtitle">我的世界</sub><div class="site-description">踏着梦走过时光</div></div><nav class="site-state"><a class="site-state-item hty-icon-button icon-home" href="/" title="首页"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:home-4-line"></span></span></a><div class="site-state-item"><a href="/archives/" title="归档"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:archive-line"></span></span><span class="site-state-item-count">6</span></a></div><div class="site-state-item"><a href="/categories/" title="分类"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:folder-2-line"></span></span><span class="site-state-item-count">1</span></a></div><div class="site-state-item"><a href="/tags/" title="标签"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:price-tag-3-line"></span></span><span class="site-state-item-count">1</span></a></div><a class="site-state-item hty-icon-button" target="_blank" rel="noopener" href="https://yun.yunyoujun.cn" title="文档"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:settings-line"></span></span></a></nav><hr style="margin-bottom:0.5rem"><div class="links-of-author"><a class="links-of-author-item hty-icon-button" rel="noopener" href="/atom.xml" title="RSS" target="_blank" style="color:orange"><span class="icon iconify" data-icon="ri:rss-line"></span></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://github.com/Wanglihan954" title="GitHub" target="_blank" style="color:#181717"><span class="icon iconify" data-icon="ri:github-line"></span></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="mailto:admin@yemengstar.com" title="E-Mail" target="_blank" style="color:#8E71C1"><span class="icon iconify" data-icon="ri:mail-line"></span></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://music.163.com/#/user/home?id=1485292158" title="网易云音乐" target="_blank" style="color:#C10D0C"><span class="icon iconify" data-icon="ri:netease-cloud-music-line"></span></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://www.zhihu.com/people/xiao-wang-52-36-47" title="知乎" target="_blank" style="color:#0084FF"><span class="icon iconify" data-icon="ri:zhihu-line"></span></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://space.bilibili.com/435092993" title="哔哩哔哩" target="_blank" style="color:#FF8EB3"><span class="icon iconify" data-icon="ri:bilibili-line"></span></a></div><hr style="margin:0.5rem 1rem"><div class="links"><a class="links-item hty-icon-button" href="/links/" title="我的小伙伴们" style="color:dodgerblue"><span class="icon iconify" data-icon="ri:genderless-line"></span></a></div><br><a class="links-item hty-icon-button" id="toggle-mode-btn" href="javascript:;" title="Mode" style="color: #f1cb64"><span class="icon iconify" data-icon="ri:contrast-2-line"></span></a></div><div class="sidebar-panel sidebar-panel-active" id="post-toc-wrap"><div class="post-toc"><div class="post-toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E5%9B%9B%E8%AF%BE-%E8%AE%A1%E7%AE%97%E5%9B%BE%E7%9A%84%E6%9E%84%E5%BB%BA"><span class="toc-number">1.</span> <span class="toc-text">第四课 计算图的构建</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8B%93%E6%89%91%E6%8E%92%E5%BA%8F"><span class="toc-number">1.1.</span> <span class="toc-text">拓扑排序</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E4%BC%98%E5%85%88%E7%9A%84%E6%8B%93%E6%89%91%E6%8E%92%E5%BA%8F%E8%AE%A1%E7%AE%97%E6%AD%A5%E9%AA%A4"><span class="toc-number">1.2.</span> <span class="toc-text">基于深度优先的拓扑排序计算步骤</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%80%E4%B8%AA%E5%A4%8D%E6%9D%82%E7%82%B9%E7%9A%84%E4%BE%8B%E5%AD%90"><span class="toc-number">1.2.1.</span> <span class="toc-text">一个复杂点的例子</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E7%9A%84-Build-%E6%9E%84%E5%BB%BA"><span class="toc-number">1.3.</span> <span class="toc-text">模型的 Build(构建)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%8A%B6%E6%80%81"><span class="toc-number">1.3.1.</span> <span class="toc-text">模型的状态</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%A7%E7%BB%AD%E6%9E%84%E5%BB%BA%E5%9B%BE%E5%85%B3%E7%B3%BB"><span class="toc-number">1.3.2.</span> <span class="toc-text">继续构建图关系</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%8A%82%E7%82%B9%E8%BE%93%E5%87%BA%E5%BC%A0%E9%87%8F%E7%9A%84%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-number">1.3.3.</span> <span class="toc-text">节点输出张量的初始化</span></a></li></ol></li></ol></li></ol></div></div></div></aside><main class="sidebar-translate" id="content"><div id="post"><article class="hty-card post-block" itemscope itemtype="https://schema.org/Article" style="--smc-primary:#0078E7;"><link itemprop="mainEntityOfPage" href="https://20020730.xyz/2025/12/03/%E7%AC%AC4%E8%AF%BE%20%E8%AE%A1%E7%AE%97%E5%9B%BE%E7%9A%84%E6%9E%84%E5%BB%BA/"><span hidden itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="name" content="宁翰"><meta itemprop="description"></span><span hidden itemprop="publisher" itemscope itemtype="https://schema.org/Organization"><meta itemprop="name" content="宁翰のHEXO"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">第4课</h1><div class="post-meta"><div class="post-time"><span class="post-meta-item-icon"><span class="icon iconify" data-icon="ri:calendar-line"></span></span> <time title="创建时间：2025-12-03 20:00:39" itemprop="dateCreated datePublished" datetime="2025-12-03T20:00:39+08:00">2025-12-03</time><span class="post-meta-divider">-</span><span class="post-meta-item-icon"><span class="icon iconify" data-icon="ri:calendar-2-line"></span></span> <time title="修改时间：2025-12-09 22:47:36" itemprop="dateModified" datetime="2025-12-09T22:47:36+08:00">2025-12-09</time></div><div class="post-classify"><span class="post-category"> <span class="post-meta-item-icon" style="margin-right:3px;"><span class="icon iconify" data-icon="ri:folder-line"></span></span><span itemprop="about" itemscope itemtype="https://schema.org/Thing"><a class="category-item" href="/categories/KuiperInfer%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" style="--text-color:var(--hty-text-color)" itemprop="url" rel="index"><span itemprop="text">KuiperInfer学习笔记</span></a></span></span></div></div></header><section class="post-body" itemprop="articleBody"><div class="post-content markdown-body"><h1 id="第四课-计算图的构建"><a href="#第四课-计算图的构建" class="headerlink" title="第四课 计算图的构建"></a>第四课 计算图的构建</h1><h2 id="拓扑排序"><a href="#拓扑排序" class="headerlink" title="拓扑排序"></a>拓扑排序</h2><p><img src="https://pica.zhimg.com/v2-37ed6844956354c8bad07d70723a1794_1440w.jpg" alt="img" loading="lazy"></p>
<p>对于一个有向无环图，拓扑排序总能够找到一个 <strong>节点序列</strong>，在这个序列中，<strong>每个节点的前驱节点都能排在这个节点的前面</strong>。什么是 <a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=233781267&content_type=Article&match_order=2&q=%E5%89%8D%E9%A9%B1%E8%8A%82%E7%82%B9&zhida_source=entity">前驱节点</a> 呢，也就是对于 <strong>有向图中任意一条边的起点，我们可以认为它是 <a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=233781267&content_type=Article&match_order=1&q=%E7%BB%88%E7%82%B9%E8%8A%82%E7%82%B9&zhida_source=entity">终点节点</a> 的前驱节点。</strong></p>
<p>对于上图例子中的深度学习模型，<code>conv1</code> 是 <code>conv2</code> 的前驱节点，<code>conv1</code> 也是 <code>conv3</code> 的前驱节点，所以在计算执行节点序列时，<code>conv1</code> 必须出现在 <code>conv2</code> 和 <code>conv3</code> 的前面，而 <code>conv2</code> 和 <code>conv3</code> 之间的顺序因为没有节点之间的连接而不做要求。因此执行节点顺序有以下两种：</p>
<ul>
<li>conv1-&gt; conv2-&gt; conv3</li>
<li>conv1-&gt; conv3-&gt; conv2</li>
</ul>
<h2 id="基于深度优先的拓扑排序计算步骤"><a href="#基于深度优先的拓扑排序计算步骤" class="headerlink" title="基于深度优先的拓扑排序计算步骤"></a>基于深度优先的拓扑排序计算步骤</h2><p>有计算排序的函数为 <code>ReverseTopo</code>. <code>ReverseTopo</code> 有参数 <code>current_op</code>.</p>
<ol>
<li>选定一个入度为零的节点(<code>current_op</code>)，入度为零指的是 <strong>该节点没有前驱节点或所有前驱节点已经都被执行过</strong>，在选定的同时将该节点的已执行标记置为 <code>True</code>，并将该节点传入到 <code>ReverseTopo</code> 函数中；</li>
<li>遍历 1 步骤中节点的 <a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=233781267&content_type=Article&match_order=1&q=%E5%90%8E%E7%BB%A7%E8%8A%82%E7%82%B9&zhida_source=entity">后继节点</a>(<code>current_op-&gt;output_operators</code>)；</li>
<li>如果 1 的某个后继节点没有被执行过（已执行标记为 <code>False</code>），则 <a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=233781267&content_type=Article&match_order=1&q=%E9%80%92%E5%BD%92&zhida_source=entity">递归</a> 将 <strong>该后继节点</strong> 传入到 <code>ReverseTopo</code> 函数中；</li>
<li>第 2 步中的遍历结束后，我们将当前节点放入到执行队列(<code>topo_operators_</code>)中。</li>
</ol>
<p>当该函数结束后，我们对 <strong><a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=233781267&content_type=Article&match_order=2&q=%E6%89%A7%E8%A1%8C%E9%98%9F%E5%88%97&zhida_source=entity">执行队列</a> 中的排序结果做逆序就得到了最终拓扑排序的结果</strong>，我们来看看具体的代码：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">RuntimeGraph::ReverseTopo</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> std::shared_ptr&lt;RuntimeOperator&gt;&amp; current_op)</span> </span>&#123;</span><br><span class="line">  <span class="built_in">CHECK</span>(current_op != <span class="literal">nullptr</span>) &lt;&lt; <span class="string">&quot;current operator is nullptr&quot;</span>;</span><br><span class="line">  current_op-&gt;has_forward = <span class="literal">true</span>;</span><br><span class="line">  <span class="type">const</span> <span class="keyword">auto</span>&amp; next_ops = current_op-&gt;output_operators;</span><br><span class="line">  <span class="keyword">for</span> (<span class="type">const</span> <span class="keyword">auto</span>&amp; [_, op] : next_ops) &#123;</span><br><span class="line">    <span class="keyword">if</span> (op != <span class="literal">nullptr</span>) &#123;</span><br><span class="line">      <span class="keyword">if</span> (!op-&gt;has_forward) &#123;</span><br><span class="line">        <span class="keyword">this</span>-&gt;<span class="built_in">ReverseTopo</span>(op);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">for</span> (<span class="type">const</span> <span class="keyword">auto</span>&amp; [_, op] : next_ops) &#123;</span><br><span class="line">    <span class="built_in">CHECK_EQ</span>(op-&gt;has_forward, <span class="literal">true</span>);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">this</span>-&gt;topo_operators_.<span class="built_in">push_back</span>(current_op);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="一个复杂点的例子"><a href="#一个复杂点的例子" class="headerlink" title="一个复杂点的例子"></a>一个复杂点的例子</h3><p><img src="https://pic4.zhimg.com/v2-e20ad84bd6a3b0c9198388b30766756f_1440w.jpg" alt="img" loading="lazy"></p>
<p>对于上图的这个例子，<code>input</code> 节点是其中唯一的输入节点，<code>output</code> 节点是其中唯一的输出节点，其他都是 <a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=233781267&content_type=Article&match_order=7&q=%E8%AE%A1%E7%AE%97%E8%8A%82%E7%82%B9&zhida_source=entity">计算节点</a>（<code>RuntieOperator</code>），表示深度学习模型中的一个算子。我们下面就对这个模型进行拓扑排序来求得它正确的执行顺序。要注意的是，<strong>拓扑排序可以有多种结果的序列，不同种序列之间的顺序也可能不完全相同，但是不同种序列均不会影响模型最终的预测输出。</strong></p>
<p>我们对上图的深度学习模型进行手工计算，计算步骤符合上一小节中的代码 <code>ReverseTopo</code> 函数。</p>
<ol>
<li>首选，传入到 <code>ReverseTopo</code> 函数中的是 <code>input</code> 节点；</li>
<li>对 <code>input</code> 节点进行遍历，<code>input</code> 节点的后继节点中没有执行过的只有 <code>op1</code>，所以我们 <strong>以递归的方式</strong> 将 <code>op1</code> 传入到 <code>ReverseTopo</code> 函数中；</li>
<li>对 <code>op1</code> 节点进行遍历，<code>op1</code> 有后继节点为 <code>op3</code> 没有被访问过，所以我们以递归的形式将 <code>op3</code> 传入到 <code>ReverseTopo</code> 函数中；</li>
<li>对 <code>op3</code> 节点进行遍历，分别有后继节点 <code>op4</code> 和 <code>op5</code>，均没有被访问过，所以我们都以递归的形式将 <code>op4</code> 传入到 <code>ReverseTopo</code> 函数当中；</li>
<li>因为 <code>op4</code> 没有后继节点，会跳出<strong>for (const auto&amp; [_, op] : next_ops)<strong>循环，所以会</strong>将本节点放入到执行队列中</strong>(<code>op4</code>)。随后该栈函数返回，<strong>返回到 4 步中以递归的形式将 <code>op5</code> 传入到 <code>ReverseTopo</code> 函数中；</strong></li>
<li><code>op5</code> 的后继节点进行遍历，后继节点为 <code>output</code> 节点，<code>output</code> 没有被访问过，所以再以递归的形式将 <code>op5</code> 传入到 <code>Reversetopo</code> 函数中。</li>
<li>最后由于 <code>output</code> 节点没有后继节点，我们将 <code>output</code> 节点 <strong>放入到执行队列中</strong>(<code>op4</code>, <code>output</code>)。随后，该栈函数返回，返回到 6 中，因为 <code>op5</code> 的后继节点已经都被访问过，所以将 <code>op5</code> 放入到执行队列中(<code>op4</code>, <code>output</code>, <code>op5</code>)。</li>
<li>从 6 再返回到 4 中，因为 <code>op3</code> 的所有后继都已经被访问过，所以将 <code>op3</code> <strong>放入到执行队列中</strong>(<code>op4</code>, <code>output</code>, <code>op5</code>, <code>op3</code>).</li>
<li>从 4 返回到 3 中，因为 <code>op1</code> 的后继节点已经都被访问过，所以将 <code>op1</code> 放入到执行队列中(<code>op4</code>, <code>output</code>, <code>op5</code>, <code>op3</code>, <code>op1</code>).</li>
<li>从 3 返回到 2 中，将 <code>input</code> 节点 <strong>放入到执行队列中</strong>，有(<code>op4</code>, <code>output</code>, <code>op5</code>, <code>op3</code>, <code>op1</code>, <code>input</code>).</li>
<li>最后需要对执行队列来一个逆序并得到最后的结果：(<code>input</code>, <code>op1</code>, <code>op3</code>, <code>op5</code>, <code>output</code>, <code>op4</code>).</li>
</ol>
<h2 id="模型的-Build-构建"><a href="#模型的-Build-构建" class="headerlink" title="模型的 Build(构建)"></a>模型的 Build(构建)</h2><p>我们在上小节中看到了 <code>model.build</code> 的调用，所以我们将在这一节中对 <code>build</code> 函数做一个补充调用。</p>
<h3 id="模型的状态"><a href="#模型的状态" class="headerlink" title="模型的状态"></a>模型的状态</h3><p><code>RuntimeGraph</code> 共有三个状态，表示 <strong>不同状态下的同一个模型</strong>（待初始化、待构建和构建完成），分别如下：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">enum class</span> <span class="title class_">GraphState</span> &#123;</span><br><span class="line">    NeedInit = <span class="number">-2</span>,</span><br><span class="line">    NeedBuild = <span class="number">-1</span>,</span><br><span class="line">    Complete = <span class="number">0</span>,</span><br><span class="line">  &#125;;</span><br></pre></td></tr></table></figure>

<p>在 <code>RuntimeGraph</code> 类中有一个变量会记录此刻模型的状态：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">GraphState graph_state_ = GraphState::NeedInit;</span><br></pre></td></tr></table></figure>

<p>三者的状态变换如下，<strong>依次表示待初始化，待构建和模型构建完成。</strong></p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">NeedInit --&gt; NeedBuild --&gt; Complete</span><br></pre></td></tr></table></figure>

<p><strong>在初始情况下</strong> 模型的状态 <code>graph_state_</code> 为 <code>NeedInit</code>，表示模型目前 <strong>待初始化</strong>。因此我们不能在此刻直接调用 <code>Build</code> 函数中的功能，<strong>而是需要在此之前先调用模型的 <code>Init</code> 函数</strong>，这一过程在下方的代码中也有体现，在初始化函数(<code>Init</code>)调用成功后会将模型的状态调整为 <code>NeedBuild</code>.</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">RuntimeGraph::Build</span><span class="params">(<span class="type">const</span> std::string&amp; input_name,</span></span></span><br><span class="line"><span class="params"><span class="function">                         <span class="type">const</span> std::string&amp; output_name)</span> </span>&#123;  </span><br><span class="line"> <span class="keyword">if</span> (graph_state_ == GraphState::Complete) &#123;</span><br><span class="line">    <span class="built_in">LOG</span>(INFO) &lt;&lt; <span class="string">&quot;Model has been built already!&quot;</span>;</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (graph_state_ == GraphState::NeedInit) &#123;</span><br><span class="line">    <span class="type">bool</span> init_graph = <span class="built_in">Init</span>();</span><br><span class="line">    <span class="built_in">LOG_IF</span>(FATAL, !init_graph) &lt;&lt; <span class="string">&quot;Init graph failed!&quot;</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="built_in">CHECK</span>(graph_state_ &gt;= GraphState::NeedBuild)</span><br></pre></td></tr></table></figure>

<p>以上构建(<code>Build</code>)函数中代码的目的是为了检查模型是否已经构建完成。也就是检查 <code>graph_state_ == GraphState::Complete</code>. 如果是这样的话，表示模型已经构建完成，<code>Build</code> 函数直接返回。如果模型此刻的状态是 <code>NeedInit</code>, 那我们首先 <strong>需要先对这个模型进行初始化</strong>（先调用 <code>Init</code> 函数），再进行构建。</p>
<p>我们再来看一下在 <code>Init</code> 函数在本节课中新增加的状态转换，可以看到在 <code>Init</code> 函数的最后，我们将模型的状态从 <code>NeedInit</code> 调整到 <code>NeedBuild</code>(需要被构建)，所以从 <code>Init</code> 函数返回之后，<code>Build</code> 函数便可以继续执行其中的代码。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">bool</span> <span class="title">RuntimeGraph::Init</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    ...</span><br><span class="line">    ...</span><br><span class="line">    graph_state_ = GraphState::NeedBuild;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br></pre></td></tr></table></figure>

<h3 id="继续构建图关系"><a href="#继续构建图关系" class="headerlink" title="继续构建图关系"></a>继续构建图关系</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">RuntimeGraph::Build</span><span class="params">(<span class="type">const</span> std::string&amp; input_name,</span></span></span><br><span class="line"><span class="params"><span class="function">                         <span class="type">const</span> std::string&amp; output_name)</span></span>&#123;</span><br><span class="line"> ...</span><br><span class="line"> ...</span><br><span class="line"> <span class="keyword">for</span> (<span class="type">const</span> <span class="keyword">auto</span>&amp; current_op : <span class="keyword">this</span>-&gt;operators_) &#123;</span><br><span class="line">    <span class="type">const</span> std::vector&lt;std::string&gt;&amp; output_names = current_op-&gt;output_names;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">const</span> <span class="keyword">auto</span>&amp; kOutputName : output_names) &#123;</span><br><span class="line">      <span class="keyword">if</span> (<span class="type">const</span> <span class="keyword">auto</span>&amp; output_op = <span class="keyword">this</span>-&gt;operators_maps_.<span class="built_in">find</span>(kOutputName);</span><br><span class="line">          output_op != <span class="keyword">this</span>-&gt;operators_maps_.<span class="built_in">end</span>()) &#123;</span><br><span class="line">        current_op-&gt;output_operators.<span class="built_in">insert</span>(&#123;kOutputName, output_op-&gt;second&#125;);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>我们继续往下看 <code>Build</code> 函数中的内容，这段代码主要用于 <strong>构建算子之间的联系</strong>。例如对于以下的深度学习模型结构，我们需要使用最外层的 <code>for</code> 循环遍历模型中的每一个算子 <code>current_op</code>.</p>
<p><img src="https://pica.zhimg.com/v2-a14969fc78f3a82ffa8a6831f1fc546a_1440w.jpg" alt="img" loading="lazy"></p>
<p>随后我们程序会拿到每个算子中的 <code>output_names</code>，例如对于 <code>op3</code> 来说，它的 <code>output_names</code> 包括了 <code>op4</code> 和 <code>op5</code>.</p>
<p>我们会对该算子(<code>op3</code>)的 <code>output_names</code> 进行遍历，并会根据 <code>output_name</code> 找到对应的后继算子并插入到 <code>op3</code> 的 <code>output_operators</code> 中。例如对于 <code>op3</code> 算子，我们会根据 <code>output_names</code> 中的 <code>&quot;op4&quot;</code> 和 <code>&quot;op5&quot;</code> 来寻找它的两个后继节点，寻找到后放入到当前计算节点 <code>current_op</code> 的后继节点列表 <code>output_operators</code> 中。</p>
<p>当最外层的循环结束时，<code>op1</code> 中的 <code>output_operators</code> 中存放了 <strong>它的一个后继节点</strong> <code>{&quot;op3&quot;:op3}</code>, 而 <code>op3</code> 中的 <code>output_operators</code> 会存放 <strong>它的两个后继节点</strong> <code>{&quot;op4&quot;:op4, &quot;op5&quot;:op5}</code>. 以此类推。不难看出，<code>output_operators</code> 中存放的是该节点所有后继算子节点的 <code>name</code> 到后继算子本身的映射。</p>
<h3 id="节点输出张量的初始化"><a href="#节点输出张量的初始化" class="headerlink" title="节点输出张量的初始化"></a>节点输出张量的初始化</h3><p>我们在 <code>Build</code> 函数中还需要 <strong>完成计算节点中输出 <a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=233781267&content_type=Article&match_order=1&q=%E5%BC%A0%E9%87%8F%E7%A9%BA%E9%97%B4&zhida_source=entity">张量空间</a> 的初始化</strong>，从下图中可以看出每个节点都有一个形状不同的输出张量，用来存放该节点的计算输出。那我们为什么要在构建(<code>Build</code>)阶段就初始化这些数据，而不是在算子计算的时候再对输出空间初始化呢？这是因为 <strong>申请较大字节数的内存空间也需要一定的时间</strong>，<strong>如果放在构建阶段进行提前申请可以在【运行时】省下这段时间。</strong></p>
<p>另外，我们需要对算子的 <strong>输出空间</strong> 做初始化和提前申请，那么我们需要对一个算子的 <strong>输入空间初始化</strong> 吗？<strong>其实是不用的</strong>，我们借用下方的这张图，<strong>节点旁的维度表示该节点输出维度大小</strong>。</p>
<p>在 <code>op1</code> 节点中我们需要申请 <code>3 x 224 x 224</code> 大小的输出空间，对于它的后继节点 <code>op3</code>，我们需要申请 <code>3 x 64 x 64</code> 的内存作为输出空间，<strong>而 <code>op3</code> 的输入空间和 <code>op1</code> 的输出空间大小相同，所以 <code>op3</code> 的输入空间可以复用前驱节点(<code>op3</code>)中的输出张量空间。</strong> 同理对于 <code>add</code> 节点，我们需要为它申请 <code>3 x 32 x 32</code> 大小的输出空间，<strong>但是对于 <code>add</code> 节点的两个输入，我们可以复用前驱节点 <code>op4</code> 和 <code>op5</code> 中的输出</strong>。</p>
<p><img src="https://picx.zhimg.com/v2-924735125d79fdadc5f8191a00080e7f_1440w.jpg" alt="img" loading="lazy"></p>
<p>以上的张量空间预分配的叙述过程对应有以下的代码：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">RuntimeOperatorUtils::InitOperatorOutput</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> std::vector&lt;pnnx::Operator*&gt;&amp; pnnx_operators,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> std::vector&lt;std::shared_ptr&lt;RuntimeOperator&gt;&gt;&amp; operators)</span> </span>&#123;</span><br><span class="line">  <span class="built_in">CHECK</span>(!pnnx_operators.<span class="built_in">empty</span>() &amp;&amp; !operators.<span class="built_in">empty</span>());</span><br><span class="line">  <span class="built_in">CHECK</span>(pnnx_operators.<span class="built_in">size</span>() == operators.<span class="built_in">size</span>());</span><br></pre></td></tr></table></figure>

<p>我们为 <code>RuntimeOperator</code> 初始化空间时，还是需要用到 <code>PNNX</code> 中的 <code>Operator</code> 结构，所以我们首先 <strong>需要判断两个数组是否是等长的</strong>，它们具有一一对应的关系。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">RuntimeOperatorUtils::InitOperatorOutput</span><span class="params">(...)</span></span>&#123;</span><br><span class="line"> ...</span><br><span class="line"> ...</span><br><span class="line"> <span class="keyword">for</span> (<span class="type">uint32_t</span> i = <span class="number">0</span>; i &lt; pnnx_operators.<span class="built_in">size</span>(); ++i) &#123;</span><br><span class="line">    <span class="comment">// 得到pnnx原有的输出空间</span></span><br><span class="line">    <span class="type">const</span> std::vector&lt;pnnx::Operand*&gt; operands = pnnx_operators.<span class="built_in">at</span>(i)-&gt;outputs;</span><br><span class="line">    <span class="built_in">CHECK</span>(operands.<span class="built_in">size</span>() &lt;= <span class="number">1</span>) &lt;&lt; <span class="string">&quot;Only support one node one output yet!&quot;</span>;</span><br><span class="line">    <span class="keyword">if</span> (operands.<span class="built_in">empty</span>()) &#123;</span><br><span class="line">      <span class="keyword">continue</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">CHECK</span>(operands.<span class="built_in">size</span>() == <span class="number">1</span>) &lt;&lt; <span class="string">&quot;Only support one output in the KuiperInfer&quot;</span>;</span><br><span class="line"></span><br><span class="line">    pnnx::Operand* operand = operands.<span class="built_in">front</span>();</span><br><span class="line">    <span class="type">const</span> <span class="keyword">auto</span>&amp; runtime_op = operators.<span class="built_in">at</span>(i);</span><br><span class="line">    <span class="built_in">CHECK</span>(operand != <span class="literal">nullptr</span>) &lt;&lt; <span class="string">&quot;Operand output is null&quot;</span>;</span><br><span class="line">    <span class="type">const</span> std::vector&lt;<span class="type">int32_t</span>&gt;&amp; operand_shapes = operand-&gt;shape;</span><br><span class="line">    <span class="type">const</span> <span class="keyword">auto</span>&amp; output_tensors = runtime_op-&gt;output_operands;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">CHECK</span>(operand != <span class="literal">nullptr</span>) &lt;&lt; <span class="string">&quot;Operand output is null&quot;</span>;</span><br><span class="line">    <span class="type">const</span> std::vector&lt;<span class="type">int32_t</span>&gt;&amp; operand_shapes = operand-&gt;shape;</span><br></pre></td></tr></table></figure>

<p>在以上的代码中，我们首先使用 <code>pnnx_operators.at(i)-&gt;outputs</code> 获得第 i 个计算节点中的所有输出计算数 <code>operand</code>, <strong>我们需要根据这个 <code>pnnx</code> 计算数 <code>Operand</code> 中记录的 <code>Shape</code> 和 <code>Type</code> 信息来初始化我们 <code>runtime_op</code> 中输出 <a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=233781267&content_type=Article&match_order=1&q=%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8&zhida_source=entity">数据存储</a> 的空间 <code>output_tensors = runtime_op-&gt;output_operands.</code></strong></p>
<p>换句话说，我们就是要根据 <code>pnnx::operand</code> 中记录的输出节点大小(<code>operand-&gt;shape</code>)来 <strong>初始化该计算节点的输出张量</strong> <code>output_tensors = runtime_op-&gt;output_operands</code> 也就是 <strong>计算节点中的具体存储空间</strong>，以及输出张量中的其他变量。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">const</span> <span class="type">int32_t</span> batch = operand_shapes.<span class="built_in">at</span>(<span class="number">0</span>);</span><br><span class="line">    <span class="built_in">CHECK</span>(batch &gt;= <span class="number">0</span>) &lt;&lt; <span class="string">&quot;Dynamic batch size is not supported!&quot;</span>;</span><br><span class="line">    <span class="built_in">CHECK</span>(operand_shapes.<span class="built_in">size</span>() == <span class="number">2</span> || operand_shapes.<span class="built_in">size</span>() == <span class="number">4</span> ||</span><br><span class="line">          operand_shapes.<span class="built_in">size</span>() == <span class="number">3</span>)</span><br><span class="line">        &lt;&lt; <span class="string">&quot;Unsupported shape sizes: &quot;</span> &lt;&lt; operand_shapes.<span class="built_in">size</span>();</span><br></pre></td></tr></table></figure>

<p>从上文知道，我们需要根据计算数的形状 <code>operand_shapes = operand-&gt;shape</code> 来初始化该计算节点的输出张量 <code>output_tensors</code> 中的存储空间，但是我们输出张量的维度只支持二维的、三维以及四维的，所以需要在以上代码上做 <code>check</code>.</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (!output_tensors) &#123;</span><br><span class="line">      <span class="comment">// 需要被初始化的输出张量</span></span><br><span class="line">      std::shared_ptr&lt;RuntimeOperand&gt; output_operand =</span><br><span class="line">          std::<span class="built_in">make_shared</span>&lt;RuntimeOperand&gt;();</span><br><span class="line">      <span class="comment">// 将输出操作数赋变量</span></span><br><span class="line">      output_operand-&gt;shapes = operand_shapes;</span><br><span class="line">      output_operand-&gt;type = RuntimeDataType::kTypeFloat32;</span><br><span class="line">      output_operand-&gt;name = operand-&gt;name + <span class="string">&quot;_output&quot;</span>;</span><br></pre></td></tr></table></figure>

<p>如果输出张量 <code>output_tensors</code> 没有被初始化，我们首先需要根据 <code>pnnx</code> 中的信息来 <strong>初始化一个保存输出张量的结构(<code>output_operand</code>)</strong>, 在这个结构中需要 <strong>保存输出张量相关的类型、名字以及维度信息</strong>。该结构的具体定义，我们已经在第二节中讲述过了，不清楚的同学可以去翻翻那节课的视频和代码。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">RuntimeOperand</span> &#123;</span><br><span class="line">  std::string name;                                     <span class="comment">/// 操作数的名称</span></span><br><span class="line">  std::vector&lt;<span class="type">int32_t</span>&gt; shapes;                          <span class="comment">/// 操作数的形状</span></span><br><span class="line">  std::vector&lt;std::shared_ptr&lt;Tensor&lt;<span class="type">float</span>&gt;&gt;&gt; datas;    <span class="comment">/// 存储操作数</span></span><br><span class="line">  RuntimeDataType type = RuntimeDataType::kTypeUnknown; <span class="comment">/// 操作数的类型</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>

<p>在上面的代码中，我们已经初始化了 <code>RuntimeOperand</code> 结构中的名字、类型和维度等信息。下面我们要去初始化结构中 <strong>存放输出数据的 <code>datas</code> 变量</strong>，它是一个张量的数组类型，数组的长度等于该计算节点的 <code>batch_size</code> 大小。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; batch; ++j) &#123;</span><br><span class="line">        <span class="keyword">if</span> (operand_shapes.<span class="built_in">size</span>() == <span class="number">4</span>) &#123;</span><br><span class="line">          sftensor output_tensor = <span class="built_in">TensorCreate</span>(</span><br><span class="line">              operand_shapes.<span class="built_in">at</span>(<span class="number">1</span>), operand_shapes.<span class="built_in">at</span>(<span class="number">2</span>), operand_shapes.<span class="built_in">at</span>(<span class="number">3</span>));</span><br><span class="line">          output_operand-&gt;datas.<span class="built_in">push_back</span>(output_tensor);</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (operand_shapes.<span class="built_in">size</span>() == <span class="number">2</span>) &#123;</span><br><span class="line">          sftensor output_tensor = <span class="built_in">TensorCreate</span>(</span><br><span class="line">              std::vector&lt;<span class="type">uint32_t</span>&gt;&#123;(<span class="type">uint32_t</span>)operand_shapes.<span class="built_in">at</span>(<span class="number">1</span>)&#125;);</span><br><span class="line">          output_operand-&gt;datas.<span class="built_in">push_back</span>(output_tensor);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          <span class="comment">// current shape is 3</span></span><br><span class="line">          sftensor output_tensor = <span class="built_in">TensorCreate</span>(std::vector&lt;<span class="type">uint32_t</span>&gt;&#123;</span><br><span class="line">              (<span class="type">uint32_t</span>)operand_shapes.<span class="built_in">at</span>(<span class="number">1</span>), (<span class="type">uint32_t</span>)operand_shapes.<span class="built_in">at</span>(<span class="number">2</span>)&#125;);</span><br><span class="line">          output_operand-&gt;datas.<span class="built_in">push_back</span>(output_tensor);</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      runtime_op-&gt;output_operands = std::<span class="built_in">move</span>(output_operand);</span><br></pre></td></tr></table></figure>

<p>对于一个计算算子 <code>runtime_op</code> 来说，它的输出张量数组的长度等于 <code>batch_size</code> 个，所以我们在循环中需要对 <code>batch_size</code> 个输出张量进行创建（创建的时候需要依据 <code>operand_shapes</code>, 从 <code>pnnx::operand</code> 中得到的维度）。</p>
<p>在创建完成后还需要放入到 <code>output_operand</code> 的 <code>datas</code> 变量中。在循环后结束后，<strong>我们会将初始化好的 <code>output_operands</code> 绑定到对应的计算节点中用于保存计算节点的输出数据。</strong> 至此，我们完成了计算节点中输出张量的初始化。</p>
</div></section><div id="reward-container"><span class="hty-icon-button button-glow" id="reward-button" title="打赏" onclick="var qr = document.getElementById(&quot;qr&quot;); qr.style.display = (qr.style.display === &quot;none&quot;) ? &quot;block&quot; : &quot;none&quot;;"><span class="icon iconify" data-icon="ri:hand-coin-line"></span></span><div id="reward-comment">I'm so cute. Please give me money.</div></div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者：</strong>宁翰</li><li class="post-copyright-link"><strong>本文链接：</strong><a href="https://20020730.xyz/2025/12/03/%E7%AC%AC4%E8%AF%BE%20%E8%AE%A1%E7%AE%97%E5%9B%BE%E7%9A%84%E6%9E%84%E5%BB%BA/" title="第4课">https://20020730.xyz/2025/12/03/%E7%AC%AC4%E8%AF%BE%20%E8%AE%A1%E7%AE%97%E5%9B%BE%E7%9A%84%E6%9E%84%E5%BB%BA/</a></li><li class="post-copyright-license"><strong>版权声明：</strong>本博客所有文章除特别声明外，均默认采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" target="_blank" rel="noopener" title="CC BY-NC-SA 4.0 "><span class="icon iconify" data-icon="ri:creative-commons-line"></span><span class="icon iconify" data-icon="ri:creative-commons-by-line"></span><span class="icon iconify" data-icon="ri:creative-commons-nc-line"></span><span class="icon iconify" data-icon="ri:creative-commons-sa-line"></span></a> 许可协议。</li></ul></article><div class="post-nav"><div class="post-nav-item"><a class="post-nav-prev" href="/2025/12/03/%E7%AC%AC3%E8%AF%BE%20%E8%AE%A1%E7%AE%97%E5%9B%BE%E7%9A%84%E8%AE%BE%E8%AE%A1/" rel="prev" title="第3课"><span class="icon iconify" data-icon="ri:arrow-left-s-line"></span><span class="post-nav-text">第3课</span></a></div><div class="post-nav-item"><a class="post-nav-next" href="/2025/12/03/%E7%AC%AC2%E8%AF%BE%20%E5%BC%A0%E9%87%8F%E7%9A%84%E8%AE%BE%E8%AE%A1/" rel="next" title="第2课"><span class="post-nav-text">第2课</span><span class="icon iconify" data-icon="ri:arrow-right-s-line"></span></a></div></div></div><div class="hty-card" id="comment"></div></main><footer class="sidebar-translate" id="footer"><div class="copyright"><span>&copy; 2019 – 2025 </span><span class="with-love" id="animate"><span class="icon iconify" data-icon="ri:cloud-line"></span></span><span class="author"> 宁翰</span></div><div class="powered"><span>由 <a href="https://hexo.io" target="_blank" rel="noopener">Hexo</a> 驱动 v8.1.1</span><span class="footer-separator">|</span><span>主题 - <a rel="noopener" href="https://github.com/YunYouJun/hexo-theme-yun" target="_blank"><span>Yun</span></a> v1.10.11</span></div></footer></div><a class="hty-icon-button" id="back-to-top" aria-label="back-to-top" href="#"><span class="icon iconify" data-icon="ri:arrow-up-s-line"></span><svg class="progress-circle-container" viewBox="0 0 100 100"><circle class="progress-circle" id="progressCircle" cx="50" cy="50" r="48" fill="none" stroke="#0078E7" stroke-width="2" stroke-linecap="round"></circle></svg></a></body></html>